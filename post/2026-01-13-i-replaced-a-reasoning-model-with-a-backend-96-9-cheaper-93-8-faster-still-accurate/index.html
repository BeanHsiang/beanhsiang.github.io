<!doctype html>
<html lang="zh-Hans">
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <meta name="referrer" content="no-referrer-when-downgrade">
    

    <title>我用后端替换了推理模型:成本降低 96.9%,速度提升 93.8%,准确性依然出色 | Innovation with tech</title>
    <meta property="og:title" content="我用后端替换了推理模型:成本降低 96.9%,速度提升 93.8%,准确性依然出色 - Innovation with tech">
    <meta property="og:type" content="article">
        
    <meta property="article:published_time" content='2026-01-13T12:00:00&#43;08:00'>
        
        
    <meta property="article:modified_time" content='2026-01-13T12:00:00&#43;08:00'>
        
    <meta name="Keywords" content="C#,.Net,Python,Nodejs,Java,Golang,博客,软件架构,云原生,机器学习,数据分析,RPA,自动化,技术管理">
    <meta name="description" content="我用后端替换了推理模型:成本降低 96.9%,速度提升 93.8%,准确性依然出色">
    <meta name="author" content="">
    <meta property="og:url" content="https://beanhsiang.github.io/post/2026-01-13-i-replaced-a-reasoning-model-with-a-backend-96-9-cheaper-93-8-faster-still-accurate/">
    <link rel="shortcut icon" href='/favicon.ico'  type="image/x-icon">

    <link rel="stylesheet" href='/css/normalize.css'>
    <link rel="stylesheet" href='/css/style.css'>
    <script type="text/javascript" src="//cdn.bootcdn.net/ajax/libs/jquery/3.4.1/jquery.min.js"></script>

    
    
    
        <link href="https://cdn.bootcdn.net/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" rel="stylesheet">
    
    
    
    
</head>

<body>
    <header id="header" class="clearfix">
    <div class="container">
        <div class="col-group">
            <div class="site-name ">
                
                    <a id="logo" href="https://beanhsiang.github.io/">
                        Innovation with tech
                    </a>
                
                <p class="description">一个技术老兵工作的点滴记录，专注、沟通、乐在分享!</p>
            </div>
            <div>
                <nav id="nav-menu" class="clearfix">
                    <a class="current" href="https://beanhsiang.github.io/">首页</a>
                    
                    <a  href="https://beanhsiang.github.io/tools/" title="工具">工具</a>
                    
                    <a  href="https://beanhsiang.github.io/archives/" title="归档">归档</a>
                    
                    <a  href="https://beanhsiang.github.io/about/" title="关于">关于</a>
                    
                </nav>
            </div>
        </div>
    </div>
</header>

    <div id="body">
        <div class="container">
            <div class="col-group">

                <div class="col-8" id="main">
                    
<div class="res-cons">
    
    <article class="post">
        <header>
            <h1 class="post-title">我用后端替换了推理模型:成本降低 96.9%,速度提升 93.8%,准确性依然出色</h1>
        </header>
        
  <time datetime="2026-01-13T04:00:00Z" class="post-meta meta-date dt-published">
    2026年1月13日
  </time>


<div class="post-meta meta-category">
  <span>&nbsp;|</span>
  
    <a href='/categories/Microsoft-Foundry' target="_blank">Microsoft Foundry</a>
  
</div>


        
        <div class="post-meta">
            <span id="busuanzi_container_page_pv">&nbsp;|
                <span id="busuanzi_value_page_pv"></span> <span>阅读</span>
            </span>
        </div>
        
        
        <div class="post-content">
            <p>在研究 AI Agent 的过程中,我发现了一个有趣的现象:虽然质量至关重要,但成本和执行时间(推理时间)往往成为 Agent 管道中最大的争议点。</p>
<p>像 GPT-5.2 或 Gemini 3 Pro 这样的&quot;推理&quot;大语言模型确实可以提高质量。然而,我注意到它们往往存在生成时间长和 token 使用量增加的问题,从而推高了成本和延迟。当 Agent 需要在每个任务中做出大量决策时,这种权衡就会成为一个重大瓶颈。</p>
<p>在这篇文章中,我将展示如何将 LLM 的角色限制为&quot;翻译器&quot;或&quot;编译器&quot;,并将实际的推理工作交给单独开发的逻辑算法。结果如何?我们在超越推理模型准确性的同时,实现了 96.9% 的成本降低和 93.8% 的执行时间缩短。</p>
<h2 id="实现的运营效益">实现的运营效益</h2>
<ul>
<li>总体准确性提升:89.3% → 90.2%</li>
<li>UNSAT 类别准确性提升:65.0% → 86.2%</li>
<li>API 成本降低:$29.50 → **$0.91**(降低 96.9%)</li>
<li>执行时间缩短:14.0 小时 → 52 分钟(缩短 93.8%)</li>
</ul>
<p>在这次开发中,我使用了 gpt-4o-mini。这个模型单独使用时,准确率徘徊在 50% 左右——本质上不比随机猜测好。然而,通过添加后端,我们的准确率超过了 90%,在所有测试模型(包括专用推理模型)中记录了最高的准确率。此外,这种高性能在所有类别和难度级别上都很稳定。</p>
<h2 id="对比目标">对比目标</h2>
<ul>
<li>纯 LLM(基线):o4-mini(OpenAI)、DeepSeek-R1 等多个模型。</li>
<li>混合方案:gpt-4o-mini + 后端(基于规则的解析 + CNF 形式化 + SAT 求解器)。</li>
</ul>
<h2 id="本文提供的内容">本文提供的内容</h2>
<p>如果你是一名致力于将 LLM 整合到公司工作流程或开发 Agent AI 的工程师,你可能正在纠结于模型选择以及质量、成本和延迟之间不可避免的权衡。这份工程报告涵盖:</p>
<ol>
<li>如何构建 LLM 和求解器的混合结构。</li>
<li>对成本和延迟影响最大的实施决策。</li>
<li>如何有效测量 token 使用量和执行时间。</li>
<li>如何重现&quot;纯 LLM&quot;和&quot;LLM + 后端&quot;之间的比较。</li>
</ol>
<p>我的目标不是声称后端应该完全取代推理模型。相反,我想展示的是,对于某些类别的决策问题,使用强大的后端可以让明显更小的模型在端到端性能上具有竞争力。</p>
<h2 id="在准确性成本和时间上实现顶级指标">在准确性、成本和时间上实现顶级指标</h2>
<p>&ldquo;基准测试&quot;是验证 LLM 质量最常见的方法。存在数百个基准测试来检查质量的各个方面——有些专门针对数学,有些针对特定的逻辑领域。在大多数讨论中,&ldquo;准确性&quot;是默认指标。</p>
<p>然而,对于 Agent 系统来说,准确性是必要条件,但不是充分条件。在生产实施中,系统在成本和执行时间方面也必须是可行的。</p>
<h2 id="一个有用的框架语言问题-vs-决策问题">一个有用的框架:语言问题 vs. 决策问题</h2>
<p>我发现许多&quot;LLM 任务&quot;可以分解如下:</p>
<ol>
<li>语言处理:从文本中提取结构、实体和关系。</li>
<li>决策制定:一致性检查、搜索分配和证明不可能性。</li>
</ol>
<p>LLM 在(1)方面很有效。然而,对于(2),确定性算法(如 SAT 求解器)要可靠和高效得多。</p>
<p>我在本文中采用的方法遵循一个简单的原则:&ldquo;使用 LLM 进行翻译,使用后端进行决策。&rdquo;</p>
<h2 id="satbench-概述2-分钟解释">SATBench 概述(2 分钟解释)</h2>
<p>为了测量推理质量、成本和延迟,我使用了斯坦福大学开发的&quot;SATBench&quot;基准测试。这个基准测试于 2025 年发布,专注于经典的布尔可满足性问题(SAT)。任务是确定给定的场景和一组条件是否可以同时满足(SAT),或者这是不可能的(UNSAT)。</p>
<p>我注意到 LLM 通常很难得出&quot;未找到解决方案&quot;的结论。因此,许多模型在这个基准测试的 UNSAT 问题上得分很低。在斯坦福的实验中,<code>gpt-4o-mini</code> 在 UNSAT 问题上只得了 13.2% 的分数,即使是最强大的模型 <code>o4-mini</code>,得分也只有 65.0%——仅略好于 50% 的随机猜测。相比之下,我们的混合模型达到了极高的 86.2%(在所有测试模型中最高)。</p>
<p>SATBench 很有用,因为它突出了纯 LLM 推理中的一个常见失败模式:&ldquo;UNSAT 检测&rdquo;。仅仅&quot;未找到解决方案&quot;并不能证明 UNSAT。要证明 UNSAT,你需要一个可靠的搜索/证明机制。当推理轨迹不完整时,许多模型倾向于输出 SAT(可满足)答案。</p>
<p>这与现实世界的 Agent 任务密切相关,例如检测约束、策略或需求中的矛盾。</p>
<h2 id="核心思想将-llm-视为编译器前端">核心思想:将 LLM 视为编译器前端</h2>
<p>我将 LLM 视为一个&quot;编译器前端&rdquo;,它将自然语言转换为正式的中间表示。</p>
<h3 id="llm-的角色形式化翻译">LLM 的角色:形式化(翻译)</h3>
<p>假设我们有以下条件:</p>
<blockquote>
<p>&ldquo;Blip 显示行为 2,或者 Blip 不显示行为 1。&rdquo;</p>
</blockquote>
<p>LLM 可以可靠地识别:</p>
<ul>
<li>实体:(&ldquo;Blip&rdquo;)</li>
<li>谓词:(&ldquo;显示行为 2&rdquo;/&ldquo;显示行为 1&rdquo;)</li>
<li>否定:(&ldquo;不&rdquo;)</li>
<li>逻辑连接词:(析取/或)</li>
</ul>
<p>这主要是一项翻译任务。</p>
<h3 id="后端的角色确定可满足性">后端的角色:确定可满足性</h3>
<p>一旦翻译成 CNF(合取范式)子句,SAT 求解器就可以确定性地判断 SAT/UNSAT。这避免了纯 LLM 推理在 UNSAT 情况下面临的模糊性。</p>
<p>因此,高成本的&quot;推理搜索&quot;部分从 LLM 调用路径中移除了。</p>
<h2 id="构建一个最小且实用的混合系统">构建一个最小且实用的混合系统</h2>
<p>我采用的管道配置如下:</p>
<ol>
<li>LLM 从用户那里接收自然语言问题。</li>
<li>问题类型路由器识别问题类型(例如,SMT、COP、PDDL/动作规划、LP)并将其发送到适当的解决管道。</li>
<li>在 SATBench 的情况下,所有内容都发送到 SAT 管道,在那里选择 SAT 求解器。</li>
<li>求解器确定问题是 SAT 还是 UNSAT。</li>
</ol>
<p>在这个管道中,你可以看到虽然 LLM 被用作翻译器/编译器,但它不参与决策。这使我们能够保证准确性,同时显著改善与 LLM 相关的成本和延迟。</p>
<h3 id="既然-satbench-只是-sat为什么还要包括路由">既然 SATBench 只是 SAT,为什么还要包括路由?</h3>
<p>尽管 SATBench 仅限于 SAT,但我构建路由是为了可扩展性。在真实的 Agent 系统中,任务通常混合多种推理范式(逻辑编程、一阶逻辑推理、CSP、SAT/SMT 风格的一致性检查)。路由提供了一个明确的接口,可以在以后集成其他求解器,而无需更改核心管道。</p>
<p>注意:本文中的所有实证结果都集中在 SAT 路由上。</p>
<h2 id="实施说明和关键工程决策">实施说明和关键工程决策</h2>
<p>本节解释对端到端性能产生重大影响的决策。</p>
<h3 id="从-llm-中消除数字索引生成">从 LLM 中消除数字索引生成</h3>
<p>在早期迭代中,我发现许多形式化失败不是由于误解句子,而是由于将名称映射到 <code>variable_mapping</code> 中的索引。</p>
<p>例如,如果 <code>variable_mapping</code> 定义为:</p>
<blockquote>
<p>&ldquo;外星生物 2 是 Blip&rdquo;</p>
</blockquote>
<p>如果模型错误地为 Blip 输出像 $x(1, …)$ 这样的文字,数学公式就会变成一个完全不同的问题。</p>
<p>为了解决这个问题,我改变了契约。LLM 不输出数字索引。</p>
<p>相反,LLM 输出命名文字:</p>
<div class="highlight"><div style="background-color:#f7f7f7;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#f7f7f7;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#f7f7f7;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span><span style="color:#1f2328">{</span>
</span></span><span style="display:flex;"><span>  <span style="color:#0550ae">&#34;entity&#34;</span><span style="color:#1f2328">:</span> <span style="color:#0a3069">&#34;Blip&#34;</span><span style="color:#1f2328">,</span>
</span></span><span style="display:flex;"><span>  <span style="color:#0550ae">&#34;j_label&#34;</span><span style="color:#1f2328">:</span> <span style="color:#0a3069">&#34;behavior 2&#34;</span><span style="color:#1f2328">,</span>
</span></span><span style="display:flex;"><span>  <span style="color:#0550ae">&#34;neg&#34;</span><span style="color:#1f2328">:</span> <span style="color:#cf222e">false</span>
</span></span><span style="display:flex;"><span><span style="color:#1f2328">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>然后,后端使用从 <code>variable_mapping</code> 派生的符号表将名称和标签解析为 $x(i,j,k)$。这将一个脆弱的步骤(索引分配)从概率生成转移到确定性解析。</p>
<h3 id="从-variable_mapping-构建健壮的符号表">从 <code>variable_mapping</code> 构建健壮的符号表</h3>
<p>在真实数据集中,使用了各种表达式:</p>
<ul>
<li>&ldquo;0 是 Alice&rdquo;</li>
<li>&ldquo;0 被指定为 Zorg&rdquo;</li>
<li>&ldquo;机器 2 指的是打印机&rdquo;</li>
</ul>
<p>如果解析器无法提取映射,命名文字就会变成&quot;未知实体&rdquo;,影响覆盖范围。我通过扩展映射模式、规范化大小写/空格以及添加受控回退路径来解决这个问题。</p>
<h3 id="作为覆盖机制的回退策略">作为覆盖机制的回退策略</h3>
<p>覆盖范围在操作上至关重要。如果系统无法解析命名文字,它不应该崩溃。相反,它应该:</p>
<ul>
<li>记录失败。</li>
<li>回退到更简单的形式化模式(例如,直接 $x(i,…)$ 输出)。</li>
<li>继续处理数据集。</li>
</ul>
<p>这种&quot;失败开放&quot;行为通常在具有下游评估或重试策略的 Agent 管道中是首选的。</p>
<h3 id="基于规则的解析作为一等组件">基于规则的解析作为一等组件</h3>
<p>对于匹配常见模板(析取、单元否定、蕴涵形式)的条件,基于规则的解析提供零成本、低延迟和更少的模式失败。事实上,这一层负责显著减少 LLM 调用的总数。</p>
<h3 id="结构化输出验证和有针对性的修复">结构化输出、验证和有针对性的修复</h3>
<p>结构化输出减少了歧义,但引入了实际的失败模式(小格式错误、杂散否定符号、全角字符、括号不匹配)。</p>
<p>我使用严格的模式验证、规范化和有针对性的修复规则(例如,将 ¬x(…) 转换为 {neg:true, var:&ldquo;x(…)&rdquo;})来减少可避免的失败。</p>
<h3 id="求解器后端确定性可满足性检查">求解器后端:确定性可满足性检查</h3>
<p>一旦 CNF 可用,可满足性的确定就委托给确定性求解器(DPLL)。这避免了&quot;未找到&quot;和&quot;不存在&quot;之间的混淆——这是纯 LLM 方法在 UNSAT 情况下的主要失败模式。</p>
<h2 id="测量结果和实际意义">测量、结果和实际意义</h2>
<h3 id="测量的内容">测量的内容</h3>
<p>对于纯 LLM 基线和混合系统,我都测量了:</p>
<ul>
<li>总 Token 数:输入/输出/总计</li>
<li>总运行时间</li>
<li>每个问题的平均时间</li>
<li>准确性:(SAT/UNSAT 的正确性)</li>
<li>覆盖范围:(可以生成最终标签的问题数量)</li>
</ul>
<p>当目标是优化 Agent 管道而不仅仅是最大化基准准确性时,这种检测是必不可少的。</p>
<h3 id="token-使用量细分100-个问题的子集">Token 使用量细分(100 个问题的子集)</h3>
<p>为了理解为什么成本下降如此剧烈,我详细分析了 100 个问题的子集。</p>
<ul>
<li>输入 Token:44,534(纯)vs 53,533(混合)</li>
<li>输出 Token:128,783(纯)vs 17,968(混合)</li>
<li>总 Token:173,317(纯)vs 71,501(混合)</li>
<li>LLM 调用次数:100(纯)vs 52(混合)</li>
</ul>
<p>主要区别在于输出 token 的数量,这主导了成本。</p>
<h3 id="对现实世界-agent-系统的解释">对现实世界 Agent 系统的解释</h3>
<p>这些结果表明了实际的工程权衡:</p>
<ul>
<li>如果 Agent 管道主要由迭代决策等检查主导,用后端替换推理式 LLM 调用可以显著降低成本和延迟。</li>
<li>即使是不执行&quot;推理&quot;的模型,如果它们的角色仅限于形式化,并且由确定性组件处理决策过程,也足够实用。</li>
</ul>
<h3 id="注意事项和部署考虑">注意事项和部署考虑</h3>
<p>这种方法不是万能的。主要注意事项是:</p>
<ul>
<li>形式化错误成为主要失败模式:如果自然语言无法可靠地映射到形式表示,求解器是否正确也无关紧要。</li>
<li>领域转移影响基于规则的解析:基于规则的解析适合类模板数据。在非结构化领域,&ldquo;快速路径&quot;覆盖范围会缩小,需要为更多实例进行 LLM 形式化。</li>
<li>后端选择取决于问题类别:SAT 求解器非常适合布尔代数一致性检查。真实的 Agent 可能需要其他后端,如 CSP、SMT、规划或数据库/查询引擎。</li>
</ul>
<h3 id="这种模式直接有用的地方">这种模式直接有用的地方</h3>
<p>在实际应用中,我预计这种模式最适用于:</p>
<ul>
<li>配置和兼容性检查。</li>
<li>策略/规则一致性(访问控制、合规约束)。</li>
<li>调度可行性(转换为 CSP/SAT 变体)。</li>
<li>多步骤 Agent 中高度受限工作流的编排。</li>
</ul>
<p>一般工程原则是:&ldquo;使用 LLM 将语言转换为结构,使用专门的后端来解决结构化问题。&rdquo;</p>
<h2 id="总结">总结</h2>
<p>到目前为止,基准测试主要用于测量 LLM 的&quot;智能&rdquo;。展望未来,我预计它们将越来越多地用作测量和保证成本和延迟的工具,同时保持&quot;足够的智能&quot;。</p>
<p>在这次实践中,我展示了使用更旧、更便宜、更快的模型实现更高准确性是可能的。我希望这能为你在内部业务工作流程中开发 AI Agent 或作为 SaaS 提供的 AI Agent 提供有用的参考。</p>
<h2 id="参考资料">参考资料</h2>
<ul>
<li><a href="https://github.com/Anjiang-Wei/SATBench?WT.mc_id=AI-MVP-5003172">GitHub: SATBench</a></li>
<li>Adaptive LLM-Symbolic Reasoning via Dynamic Logical Solver Composition: <a href="https://arxiv.org/html/2510.06774v1?WT.mc_id=AI-MVP-5003172">https://arxiv.org/html/2510.06774v1?WT.mc_id=AI-MVP-5003172</a></li>
<li>Automatically discovering heuristics in a complex SAT solver with large language models: <a href="https://arxiv.org/abs/2507.22876?WT.mc_id=AI-MVP-5003172">https://arxiv.org/abs/2507.22876?WT.mc_id=AI-MVP-5003172</a></li>
<li><a href="https://arxiv.org/html/2509.12602v1?WT.mc_id=AI-MVP-5003172">DaSAThco: Data-Aware SAT Heuristics Combinations Optimization via Large Language Models</a></li>
<li><a href="https://arxiv.org/abs/2509.07367?WT.mc_id=AI-MVP-5003172">Autonomous Code Evolution Meets NP-Completeness</a></li>
</ul>

        </div>

        
<div class="post-archive">
    <ul class="post-copyright">
        <li><strong>原文作者：</strong><a rel="author" href="https://beanhsiang.github.io/">BeanHsiang</a></li>
        <li style="word-break:break-all"><strong>原文链接：</strong><a href="https://beanhsiang.github.io/post/2026-01-13-i-replaced-a-reasoning-model-with-a-backend-96-9-cheaper-93-8-faster-still-accurate/">https://beanhsiang.github.io/post/2026-01-13-i-replaced-a-reasoning-model-with-a-backend-96-9-cheaper-93-8-faster-still-accurate/</a></li>
        <li><strong>版权声明：</strong>本作品采用<a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/">知识共享署名-非商业性使用-禁止演绎 4.0 国际许可协议</a>. 进行许可，非商业转载请注明出处（作者，原文链接），商业转载请联系作者获得授权。</li>
    </ul>
</div>
<br/>



        

<div class="post-archive">
    <h2>相关文章</h2>
    <ul class="listing">
        
        <li><a href="/post/2026-01-07-agentic-ai-trends-2026/">2026 年值得关注的 7 个 Agentic AI 趋势</a></li>
        
        <li><a href="/post/2026-01-06-code-review-in-the-age-of-ai/">AI 时代的代码审查</a></li>
        
        <li><a href="/post/2026-01-04-building-a-production-ready-sip-gateway-for-azure-voice-live/">为 Azure Voice Live 构建可用于生产的 SIP 网关</a></li>
        
        <li><a href="/post/2026-01-01-why-web-ai-agents-are-more-vulnerable-than-standalone-llms-and-how-we-should-actually-fix-it/">为什么 Web AI Agent 比独立 LLM 更脆弱——以及我们应该如何真正修复它</a></li>
        
        <li><a href="/post/2025-12-12-observability-for-multi-agent-systems-with-microsoft-agent-framework-and-azure-ai-foundry/">使用 Microsoft Agent Framework 与 Azure AI Foundry 为多智能体系统构建可观测性</a></li>
        
    </ul>
</div>


        <div class="post-meta meta-tags">
            
            <ul class="clearfix">
                
                <li><a href='/tags/Microsoft-Foundry' target="_blank">Microsoft Foundry</a></li>
                
            </ul>
            
        </div>
    </article>
    
    

    
    
    <div class="post bg-white">
      <script src="https://utteranc.es/client.js"
            repo= "BeanHsiang/Greedy"
            issue-term="pathname"
            theme="github-light"
            crossorigin="anonymous"
            async>
      </script>
    </div>
    
    
    

</div>

                    <footer id="footer">
    <div>
        &copy; 2026 <a href="https://beanhsiang.github.io/">Innovation with tech By BeanHsiang</a>
        
    </div>
    <br />
    <div>
        <div class="github-badge">
            <a href="https://gohugo.io/" target="_black" rel="nofollow"><span class="badge-subject">Powered by</span><span class="badge-value bg-blue">Hugo</span></a>
        </div>
        <div class="github-badge">
            <a href="https://github.com/flysnow-org/maupassant-hugo" target="_black"><span class="badge-subject">Theme</span><span class="badge-value bg-yellowgreen">Maupassant</span></a>
        </div>
    </div>
</footer>


    
    
    <script type="text/javascript">
        window.MathJax = {
            tex2jax: {
                inlineMath: [['$', '$']],
                processEscapes: true
                }
            };
    </script>
    <script src='//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script><script src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js"></script>

<a id="rocket" href="#top"></a>
<script type="text/javascript" src='/js/totop.js?v=0.0.0' async=""></script>

<style type="text/css">
div.highlight {
    position: relative;
    margin: 1em 0px;
}

.copy-code {
    display: none;
    position: absolute;
    top: 4px;
    right: 4px;
    color: rgba(255, 255, 255, 0.8);
    background: rgba(78, 78, 78, 0.8);
    border-radius: var(--radius);
    padding: 0 5px;
    font: inherit;
    user-select: none;
    cursor: pointer;
    border: 0;
    --radius: 8px;
}

div.highlight:hover .copy-code,pre:hover .copy-code {
    display: block;
}

</style>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>


    <script type="text/javascript" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>




                </div>

                <div id="secondary">
    <section class="widget">
        <form id="search" action='https://beanhsiang.github.io/search' method="get" accept-charset="utf-8" target="_blank" _lpchecked="1">
      
      <input type="text" name="q" maxlength="20" placeholder="Search">
      <input type="hidden" name="sitesearch" value="https://beanhsiang.github.io/">
      <button type="submit" class="submit icon-search"></button>
</form>
    </section>

    <section class="widget">
        <h3 class="widget-title">最近文章</h3>
<ul class="widget-list">
    
    <li>
        <a href="https://beanhsiang.github.io/post/2026-01-13-i-replaced-a-reasoning-model-with-a-backend-96-9-cheaper-93-8-faster-still-accurate/" title="我用后端替换了推理模型:成本降低 96.9%,速度提升 93.8%,准确性依然出色" target="_blank">我用后端替换了推理模型:成本降低 96.9%,速度提升 93.8%,准确性依然出色</a>
    </li>
    
    <li>
        <a href="https://beanhsiang.github.io/post/2026-01-07-agentic-ai-trends-2026/" title="2026 年值得关注的 7 个 Agentic AI 趋势" target="_blank">2026 年值得关注的 7 个 Agentic AI 趋势</a>
    </li>
    
    <li>
        <a href="https://beanhsiang.github.io/post/2026-01-06-code-review-in-the-age-of-ai/" title="AI 时代的代码审查" target="_blank">AI 时代的代码审查</a>
    </li>
    
    <li>
        <a href="https://beanhsiang.github.io/post/2026-01-06-mbml-learning-skills/" title="[译]基于模型的机器学习 - 第二章 评估人们的技能" target="_blank">[译]基于模型的机器学习 - 第二章 评估人们的技能</a>
    </li>
    
    <li>
        <a href="https://beanhsiang.github.io/post/2026-01-05-mbml-murder-mystery_extending_the_model/" title="[译]基于模型的机器学习 - 1.4 扩展模型" target="_blank">[译]基于模型的机器学习 - 1.4 扩展模型</a>
    </li>
    
    <li>
        <a href="https://beanhsiang.github.io/post/2026-01-04-building-a-production-ready-sip-gateway-for-azure-voice-live/" title="为 Azure Voice Live 构建可用于生产的 SIP 网关" target="_blank">为 Azure Voice Live 构建可用于生产的 SIP 网关</a>
    </li>
    
    <li>
        <a href="https://beanhsiang.github.io/post/2026-01-01-why-web-ai-agents-are-more-vulnerable-than-standalone-llms-and-how-we-should-actually-fix-it/" title="为什么 Web AI Agent 比独立 LLM 更脆弱——以及我们应该如何真正修复它" target="_blank">为什么 Web AI Agent 比独立 LLM 更脆弱——以及我们应该如何真正修复它</a>
    </li>
    
    <li>
        <a href="https://beanhsiang.github.io/post/2025-12-12-observability-for-multi-agent-systems-with-microsoft-agent-framework-and-azure-ai-foundry/" title="使用 Microsoft Agent Framework 与 Azure AI Foundry 为多智能体系统构建可观测性" target="_blank">使用 Microsoft Agent Framework 与 Azure AI Foundry 为多智能体系统构建可观测性</a>
    </li>
    
    <li>
        <a href="https://beanhsiang.github.io/post/2025-12-08-monitoring-generative-ai-applications-with-azure-ai-foundry/" title="使用 Azure AI Foundry 监控生成式 AI 应用" target="_blank">使用 Azure AI Foundry 监控生成式 AI 应用</a>
    </li>
    
    <li>
        <a href="https://beanhsiang.github.io/post/2025-11-22-hybrid-ai-using-foundry-local-microsoft-foundry-and-the-agent-framework-part-2/" title="使用 Foundry Local、Microsoft Foundry 与 Agent Framework 构建混合 AI（下）：云端 agent 安全回拨本地私密上下文" target="_blank">使用 Foundry Local、Microsoft Foundry 与 Agent Framework 构建混合 AI（下）：云端 agent 安全回拨本地私密上下文</a>
    </li>
    
</ul>
    </section>

    

    <section class="widget">
        <h3 class="widget-title"><a href='/categories/'>分类</a></h3>
<ul class="widget-list">
    
    <li><a href="https://beanhsiang.github.io/categories/AI-Agent/">AI Agent (3)</a></li>
    
    <li><a href="https://beanhsiang.github.io/categories/Azure-AI/">Azure AI (38)</a></li>
    
    <li><a href="https://beanhsiang.github.io/categories/FireUG/">FireUG (5)</a></li>
    
    <li><a href="https://beanhsiang.github.io/categories/GitHub-Copilot/">GitHub Copilot (2)</a></li>
    
    <li><a href="https://beanhsiang.github.io/categories/Machine-Learning/">Machine Learning (1)</a></li>
    
    <li><a href="https://beanhsiang.github.io/categories/Microsoft-Foundry/">Microsoft Foundry (15)</a></li>
    
    <li><a href="https://beanhsiang.github.io/categories/ML.NET/">ML.NET (8)</a></li>
    
    <li><a href="https://beanhsiang.github.io/categories/Model-Based-Machine-Learning/">Model-Based Machine Learning (9)</a></li>
    
    <li><a href="https://beanhsiang.github.io/categories/%E7%A9%BF%E8%B6%8A%E4%BA%BA%E6%9C%BA%E8%9E%8D%E5%90%88%E4%B9%8B%E9%99%85/">穿越人机融合之际 (6)</a></li>
    
</ul>
    </section>

    <section class="widget">
        <h3 class="widget-title"><a href='/tags/'>标签</a></h3>
<div class="tagcloud">
    
    <a href="https://beanhsiang.github.io/tags/.NET/">.NET</a>
    
    <a href="https://beanhsiang.github.io/tags/A2A/">A2A</a>
    
    <a href="https://beanhsiang.github.io/tags/Agentic-AI/">Agentic AI</a>
    
    <a href="https://beanhsiang.github.io/tags/AI-Agent/">AI Agent</a>
    
    <a href="https://beanhsiang.github.io/tags/AutoGen/">AutoGen</a>
    
    <a href="https://beanhsiang.github.io/tags/Azure-AI/">Azure AI</a>
    
    <a href="https://beanhsiang.github.io/tags/Azure-AI-Agent/">Azure AI Agent</a>
    
    <a href="https://beanhsiang.github.io/tags/Azure-AI-Search/">Azure AI Search</a>
    
    <a href="https://beanhsiang.github.io/tags/Azure-Machine-Learning/">Azure Machine Learning</a>
    
    <a href="https://beanhsiang.github.io/tags/Azure-OpenAI/">Azure OpenAI</a>
    
    <a href="https://beanhsiang.github.io/tags/Azure-Speech/">Azure Speech</a>
    
    <a href="https://beanhsiang.github.io/tags/CSnakes/">CSnakes</a>
    
    <a href="https://beanhsiang.github.io/tags/FireUG/">FireUG</a>
    
    <a href="https://beanhsiang.github.io/tags/Gemini/">Gemini</a>
    
    <a href="https://beanhsiang.github.io/tags/GitHub-Copilot/">GitHub Copilot</a>
    
    <a href="https://beanhsiang.github.io/tags/GPT-4o/">GPT-4o</a>
    
    <a href="https://beanhsiang.github.io/tags/GPT-4o-Realtime-Preview/">GPT-4o Realtime Preview</a>
    
    <a href="https://beanhsiang.github.io/tags/Machine-Learning/">Machine Learning</a>
    
    <a href="https://beanhsiang.github.io/tags/Magentic-One/">Magentic-One</a>
    
    <a href="https://beanhsiang.github.io/tags/MarkItDown/">MarkItDown</a>
    
    <a href="https://beanhsiang.github.io/tags/MCP/">MCP</a>
    
    <a href="https://beanhsiang.github.io/tags/Microsoft-Entra-ID/">Microsoft Entra ID</a>
    
    <a href="https://beanhsiang.github.io/tags/Microsoft-Foundry/">Microsoft Foundry</a>
    
    <a href="https://beanhsiang.github.io/tags/ML.NET/">ML.NET</a>
    
    <a href="https://beanhsiang.github.io/tags/MLX/">MLX</a>
    
    <a href="https://beanhsiang.github.io/tags/Model-Context-Protocol/">Model Context Protocol</a>
    
    <a href="https://beanhsiang.github.io/tags/Model-Based/">Model-Based</a>
    
    <a href="https://beanhsiang.github.io/tags/Multi-Agent/">Multi-Agent</a>
    
    <a href="https://beanhsiang.github.io/tags/Phi-3/">Phi-3</a>
    
    <a href="https://beanhsiang.github.io/tags/Phi-4/">Phi-4</a>
    
    <a href="https://beanhsiang.github.io/tags/Prompt-Engineering/">Prompt Engineering</a>
    
    <a href="https://beanhsiang.github.io/tags/PydanticAI/">PydanticAI</a>
    
    <a href="https://beanhsiang.github.io/tags/q-learning/">Q-Learning</a>
    
    <a href="https://beanhsiang.github.io/tags/RAFT/">RAFT</a>
    
    <a href="https://beanhsiang.github.io/tags/RAG/">RAG</a>
    
    <a href="https://beanhsiang.github.io/tags/Real-Time-API/">Real-Time API</a>
    
    <a href="https://beanhsiang.github.io/tags/reinforcement-learning/">Reinforcement Learning</a>
    
    <a href="https://beanhsiang.github.io/tags/Semantic-Kernel/">Semantic Kernel</a>
    
    <a href="https://beanhsiang.github.io/tags/Semantic-Ranking/">Semantic Ranking</a>
    
    <a href="https://beanhsiang.github.io/tags/Speech-Recognition/">Speech Recognition</a>
    
    <a href="https://beanhsiang.github.io/tags/Transcribe/">Transcribe</a>
    
    <a href="https://beanhsiang.github.io/tags/Vector-Search/">Vector Search</a>
    
    <a href="https://beanhsiang.github.io/tags/VoiceRAG/">VoiceRAG</a>
    
    <a href="https://beanhsiang.github.io/tags/VSCode/">VSCode</a>
    
    <a href="https://beanhsiang.github.io/tags/Websocket/">Websocket</a>
    
    <a href="https://beanhsiang.github.io/tags/%E7%A9%BF%E8%B6%8A%E4%BA%BA%E6%9C%BA%E8%9E%8D%E5%90%88%E4%B9%8B%E9%99%85/">穿越人机融合之际</a>
    
</div>
    </section>

    
<section class="widget">
    <h3 class="widget-title">友情链接</h3>
    <ul class="widget-list">
        
        <li>
            <a target="_blank" href="https://fireusergroup.com/" title="FireUG">FireUG 技术社区</a>
        </li>
        
        <li>
            <a target="_blank" href="https://space.bilibili.com/545713776" title="FireUG 技术社区 - B 站">FireUG 技术社区 - B 站</a>
        </li>
        
        <li>
            <a target="_blank" href="https://www.douyin.com/user/MS4wLjABAAAAYGG_Q3--hpBKc7rq2h-slFNFObDCmrxYc8OF2tl_mV4" title="火油鸡">火油鸡</a>
        </li>
        
    </ul>
</section>


    <section class="widget">
        <h3 class="widget-title">其它</h3>
        <ul class="widget-list">
            <li><a href="https://beanhsiang.github.io/index.xml">文章 RSS</a></li>
        </ul>
    </section>
</div>
            </div>
        </div>
    </div>
</body>

</html>