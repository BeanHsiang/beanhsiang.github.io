<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Innovation with tech</title>
    <link>https://beanhsiang.github.io/</link>
    <description>Recent content on Innovation with tech</description>
    <generator>Hugo</generator>
    <language>zh-Hans</language>
    <lastBuildDate>Thu, 08 May 2025 12:00:00 +0800</lastBuildDate>
    <atom:link href="https://beanhsiang.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>使用 Semantic Kernel 插件编排 AI 代理的技术深度解析</title>
      <link>https://beanhsiang.github.io/post/2025-05-08-a-technical-deep-dive-into-orchestrating-ai-agents-using-semantic-kernel-plugins/</link>
      <pubDate>Thu, 08 May 2025 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2025-05-08-a-technical-deep-dive-into-orchestrating-ai-agents-using-semantic-kernel-plugins/</guid>
      <description>&lt;p&gt;在如今快速发展的 &lt;strong&gt;大型语言模型（LLM）&lt;/strong&gt; 领域，编排专门的 AI 代理已成为构建复杂认知系统的关键，这些系统能够进行复杂推理和任务执行。虽然功能强大，但协调多个具有独特能力和数据访问权限的代理会带来显著的工程挑战。微软的 Semantic Kernel（SK）通过其直观的插件系统为管理这种复杂性提供了强大的框架。本文将深入探讨如何利用 SK 插件实现高效的代理编排，并结合实际实现模式进行说明。&lt;/p&gt;</description>
    </item>
    <item>
      <title>[译]基于模型的机器学习 - 1.1 整合证据</title>
      <link>https://beanhsiang.github.io/post/2025-04-28-mbml-murder-mystery_incorporating_evidence/</link>
      <pubDate>Mon, 28 Apr 2025 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2025-04-28-mbml-murder-mystery_incorporating_evidence/</guid>
      <description>&lt;p&gt;贝叶斯博士彻底搜查了整个豪宅。她发现可用的武器只有一把装饰性匕首和一把旧军用左轮手枪。&amp;ldquo;凶器一定是其中之一&amp;rdquo;，她得出结论。&lt;/p&gt;&#xA;&lt;p&gt;到目前为止，我们只考虑了一个&lt;strong&gt;随机变量&lt;/strong&gt;：&lt;code&gt;murderer&lt;/code&gt;（凶手）。但现在我们有了关于可能凶器的新信息，我们可以引入一个新的&lt;strong&gt;随机变量&lt;/strong&gt; &lt;code&gt;weapon&lt;/code&gt;（武器）来表示凶器的选择。这个新变量可以取两个值：&lt;code&gt;revolver&lt;/code&gt;（左轮手枪）或 &lt;code&gt;dagger&lt;/code&gt;（匕首）。有了这个新变量，下一步就是使用&lt;strong&gt;概率&lt;/strong&gt;来表达它与我们现有的 &lt;code&gt;murderer&lt;/code&gt; 变量之间的关系。这将让我们能够推理这些变量如何相互影响，并在破案中取得进展。&lt;/p&gt;</description>
    </item>
    <item>
      <title>使用 PydanticAI 构建智能代理系统：从 MCP 到智能工单助手</title>
      <link>https://beanhsiang.github.io/post/2025-04-24-using-pydanticai-for-agentic-development/</link>
      <pubDate>Thu, 24 Apr 2025 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2025-04-24-using-pydanticai-for-agentic-development/</guid>
      <description>&lt;p&gt;在人工智能快速发展的今天，智能代理(Agentic AI) 成为了一个热门话题。本文将介绍如何使用 PydanticAI 构建智能代理系统，包括与 MCP(Model Context Protocol) 的集成以及一个实际的智能工单助手应用案例。&lt;/p&gt;&#xA;&lt;h2 id=&#34;pydanticai-与-mcp-简介&#34;&gt;PydanticAI 与 MCP 简介&lt;/h2&gt;&#xA;&lt;p&gt;PydanticAI 是一个强大的 Python 库，它允许开发者以类型安全的方式定义和使用 AI 代理。它与 Pydantic v2 完美集成，提供了清晰的数据验证和模式定义能力。而 MCP(Model Context Protocol) 则是一个标准化协议，用于定义 LLM 如何与工具交互。&lt;/p&gt;</description>
    </item>
    <item>
      <title>[译]基于模型的机器学习 - 第一章 谋杀之谜</title>
      <link>https://beanhsiang.github.io/post/2025-04-22-mbml-murder-mystery/</link>
      <pubDate>Tue, 22 Apr 2025 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2025-04-22-mbml-murder-mystery/</guid>
      <description>&lt;p&gt;&#xA;        &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://beanhsiang.github.io/uploads/shutterstock_101951590_dark_and_stormy_print_resolution.jpg&#34;&gt;&#xA;            &lt;img class=&#34;mx-auto&#34; alt=&#34;谋杀之谜&#34; src=&#34;https://beanhsiang.github.io/uploads/shutterstock_101951590_dark_and_stormy_print_resolution.jpg&#34; /&gt;&#xA;        &lt;/a&gt;&#xA;    &lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;当老都铎王朝豪宅的午夜钟声敲响时，一场狂风暴雨嘎嘎作响地敲响了百叶窗，房子里充满了雷声。布莱克先生的尸体倒在图书馆的地板上，鲜血仍在从致命伤口中渗出。很快赶到现场的是著名的侦探贝叶斯博士，他观察到谋杀时豪宅里只有另外两个人。那么是谁犯下了这种卑鄙的罪行呢？是格雷少校那根正直的柱子吗？还是神秘而诱人的蛇蝎美人奥本小姐？&lt;/p&gt;</description>
    </item>
    <item>
      <title>[译]基于模型的机器学习 - 如何用机器学习解决我的问题？</title>
      <link>https://beanhsiang.github.io/post/2025-04-21-mbml-introduction/</link>
      <pubDate>Mon, 21 Apr 2025 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2025-04-21-mbml-introduction/</guid>
      <description>&lt;p&gt;作为机器学习研究人员，我们几乎每天都会被问到这样一个问题：&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&amp;ldquo;机器学习如何解决我的问题？&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;在本书中，我们将通过实例来回答这个问题。我们不仅仅列举机器学习技术和概念，而是通过一系列案例研究，从问题陈述到工作解决方案的全过程来说明。在解决每个问题的过程中，我们会逐步解释所涉及的机器学习概念。我们展示的案例研究都是来自微软的真实例子，以及一个介绍核心概念的初始案例研究。我们还会探讨在每个案例研究中遇到的实际问题，以及它们是如何被发现、诊断和解决的。我们的目标不仅是解释机器学习方法是什么，还要说明如何创建、调试和改进它们来解决你的问题。&lt;/p&gt;</description>
    </item>
    <item>
      <title>[译]基于模型的机器学习 - 前言</title>
      <link>https://beanhsiang.github.io/post/2025-04-20-mbml-preface/</link>
      <pubDate>Sun, 20 Apr 2025 11:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2025-04-20-mbml-preface/</guid>
      <description>&lt;p&gt;如今，机器学习正被应用于越来越多的领域，面临着各种各样的问题。在进行机器学习时，一个根本性的挑战是将某种机器学习技术的抽象数学与具体的现实问题联系起来。本书通过“基于模型的机器学习”方法来应对这一挑战。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;基于模型的机器学习&lt;/strong&gt;是一种关注于理解机器学习系统中所蕴含假设及其对系统行为影响的方法。其实践过程是将对现实世界的假设与实现机器学习所需的详细算法数学分离开来。这种方法不仅有助于理解机器学习系统的行为，也便于与他人沟通。&lt;/p&gt;</description>
    </item>
    <item>
      <title>[译]基于模型的机器学习 - 目录</title>
      <link>https://beanhsiang.github.io/post/2025-04-20-mbml-table-of-contents/</link>
      <pubDate>Sun, 20 Apr 2025 10:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2025-04-20-mbml-table-of-contents/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://beanhsiang.github.io/post/2025-04-20-mbml-preface&#34;&gt;前言&lt;/a&gt;&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;本书简介、适用读者及阅读方法&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://beanhsiang.github.io/post/2025-04-21-mbml-introduction&#34;&gt;如何用机器学习解决我的问题？&lt;/a&gt;&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;什么是基于模型的机器学习及其如何帮助解决问题&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://beanhsiang.github.io/post/2025-04-22-mbml-murder-mystery&#34;&gt;谋杀之谜&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;通过解决一起谋杀案介绍基于模型的机器学习的核心概念&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;关键词：概率、随机变量、概率推断、概率模型、因子图、贝叶斯定理&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;1.1 &lt;a href=&#34;https://beanhsiang.github.io/post/2025-04-28-mbml-murder-mystery_incorporating_evidence&#34;&gt;整合证据&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;ol start=&#34;2&#34;&gt;&#xA;&lt;li&gt;评估人的技能&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;基于模型的机器学习的第一个应用：根据测试答案评估一个人具备哪些技能&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;关键词：消息传递算法、循环置信传播、可视化、评估指标、ROC曲线&lt;/p&gt;</description>
    </item>
    <item>
      <title>使用 Semantic Kernel Python 集成 Google A2A协议</title>
      <link>https://beanhsiang.github.io/post/2025-04-18-google-a2a-protocol-integrated-using-semantic-kernel-python/</link>
      <pubDate>Fri, 18 Apr 2025 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2025-04-18-google-a2a-protocol-integrated-using-semantic-kernel-python/</guid>
      <description>&lt;p&gt;Google的Agent-to-Agent（A2A）协议旨在实现不同AI代理之间的无缝互操作性。而微软的**Semantic Kernel（SK）**是一个开源平台，用于协调智能代理的交互。本文将介绍如何将Semantic Kernel代理集成到A2A生态系统中，并作为A2A服务器高效地路由代理调用到专业服务。&lt;/p&gt;</description>
    </item>
    <item>
      <title>使用 TypeScript 构建 Azure AI Agent 的 MCP 服务器</title>
      <link>https://beanhsiang.github.io/post/2025-04-14-use-typescript-to-build-an-mcp-server-for-azure-ai-agent/</link>
      <pubDate>Mon, 14 Apr 2025 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2025-04-14-use-typescript-to-build-an-mcp-server-for-azure-ai-agent/</guid>
      <description>&lt;h2 id=&#34;简介&#34;&gt;简介&lt;/h2&gt;&#xA;&lt;p&gt;这篇文章将带你了解如何使用 TypeScript 构建一个 &lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt; 服务器，以连接 Azure AI Agent 和 Claude Desktop 或其他支持 MCP 的客户端。通过本文，你将学习搭建服务器、配置连接以及以编程方式处理 AI Agent 的交互。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;为什么需要-mcp-服务器&#34;&gt;为什么需要 MCP 服务器？&lt;/h2&gt;&#xA;&lt;p&gt;Azure AI Agent 是 &lt;strong&gt;Azure AI Foundry&lt;/strong&gt; 生态系统的一部分，提供强大的对话式 AI 功能。然而，要将这些 Agent 集成到桌面应用程序中，往往需要定制化的解决方案。MCP 提供了一个标准化协议，可以无缝连接 &lt;strong&gt;Azure AI Agent&lt;/strong&gt; 和支持 MCP 的客户端（如 Claude Desktop）。&lt;/p&gt;</description>
    </item>
    <item>
      <title>通用模型与推理模型：Azure OpenAI 的选择指南</title>
      <link>https://beanhsiang.github.io/post/2025-04-14-general-models-vs-inference-models-a-selection-guide-for-azure-openai/</link>
      <pubDate>Mon, 14 Apr 2025 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2025-04-14-general-models-vs-inference-models-a-selection-guide-for-azure-openai/</guid>
      <description>&lt;p&gt;随着大型语言模型（LLM）的普及，市场上涌现了许多不同类型的模型，满足从日常聊天到高级科学推理的各种任务需求。如果你熟悉 GPT-3.5 和 GPT-4，你会知道它们在通用 AI 领域设立了高标准。然而，随着技术的发展，模型之间的差异也变得更加显著。&lt;/p&gt;&#xA;&lt;p&gt;本文将从以下几个方面探讨通用模型与推理模型的区别，并结合 Azure OpenAI 提供的具体模型进行说明：&lt;/p&gt;</description>
    </item>
    <item>
      <title>FireUG x Global AI Bootcamp 2025</title>
      <link>https://beanhsiang.github.io/post/2025-03-22-fireug-global-ai-bootcamp-2025/</link>
      <pubDate>Sat, 15 Mar 2025 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2025-03-22-fireug-global-ai-bootcamp-2025/</guid>
      <description>&lt;h1 id=&#34;global-ai-bootcamp-2025&#34;&gt;Global AI Bootcamp 2025&lt;/h1&gt;&#xA;&lt;p&gt;&#xA;        &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://beanhsiang.github.io/uploads/aibootcamp2025.png&#34;&gt;&#xA;            &lt;img class=&#34;mx-auto&#34; alt=&#34;Bootcamp&#34; src=&#34;https://beanhsiang.github.io/uploads/aibootcamp2025.png&#34; /&gt;&#xA;        &lt;/a&gt;&#xA;    &lt;/p&gt;&#xA;&lt;p&gt;DeepSeek shook the global AI community, making Hangzhou the first city to become popular by 2025 due to open-source large models. The &amp;ldquo;Hangzhou Six Little Dragons&amp;rdquo; has become a demonstration of the vigorous vitality of the city&amp;rsquo;s scientific and technological innovation.&lt;/p&gt;&#xA;&lt;p&gt;On March 22nd, Global AI Bootcamp 2025 will come to Hangzhou, where a number of senior technical experts and developers in Hangzhou will focus on the practical application of the latest open source large models, and learn how to use AI to achieve x10 capabilities in their daily work.&lt;/p&gt;&#xA;&lt;p&gt;Join us to stay up-to-date on the latest AI technologies and explore a smarter AI future.&lt;/p&gt;&#xA;&lt;h2 id=&#34;main-organizer&#34;&gt;Main Organizer&lt;/h2&gt;&#xA;&lt;p&gt;Bin Xiang&lt;/p&gt;&#xA;&lt;p&gt;Microsoft Most Valuable Professional, One of the organizers of FireUG, Organizer at Global AI Bootcamp&lt;/p&gt;&#xA;&lt;h2 id=&#34;agenda&#34;&gt;Agenda&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;[14:00-14:15] &lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;Weiyu Xiao&lt;/p&gt;&#xA;&lt;p&gt;Microsoft Most Valuable Professional, One of the organizers of FireUG, NetCorePal Framework Developer&lt;/p&gt;</description>
    </item>
    <item>
      <title>用 CSnakes 把 MarkItDown 嵌入到 .NET 应用程序中</title>
      <link>https://beanhsiang.github.io/post/2025-03-07-embedding-markitdown-into-dotnet/</link>
      <pubDate>Fri, 07 Mar 2025 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2025-03-07-embedding-markitdown-into-dotnet/</guid>
      <description>&lt;p&gt;最近在开发智能文档搜索的工作中，我需要将各种文件转换为 Markdown 格式，然后通过向量化计算，把它们创建到向量数据库中，以便实现语义搜索文档的功能。并结合 LLMs 和 RAG（Retrieval Augmented Generation）来获取与搜索目标相关的文档内容完成二次创作。由于多模态下需要处理的文档类型太多了，我曾用 python 写了几个工具方法，使用 MarkItDown 来统一处理的。但现实问题是 MarkItDown 目前没有 .NET 版本的实现，而我的应用是 .NET 的程序，如果能有一个办法将 python 脚本无缝嵌入到我的 .NET 应用程序中那就太棒了，这就是我为什么要使用 CSnakes 的原因。&lt;/p&gt;</description>
    </item>
    <item>
      <title>基于Azure AI搜索和GPT-4o实时音频的语音化RAG</title>
      <link>https://beanhsiang.github.io/post/2025-01-06-voicerag-for-rag/</link>
      <pubDate>Mon, 06 Jan 2025 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2025-01-06-voicerag-for-rag/</guid>
      <description>&lt;p&gt;本文介绍了一种简单的语音生成式 AI 应用架构，通过结合新的 gpt-4o-realtime-preview 模型和 Azure AI Search，实现了 RAG 模式。新的 Azure OpenAI gpt-4o-realtime-preview 模型以其语音到语音的功能，为更自然的应用程序用户界面打开了大门。这种新的基于语音的界面也带来了一个有趣的新挑战：如何在使用音频作为输入和输出的系统中实现检索增强生成 (RAG)？&lt;/p&gt;&#xA;&lt;p&gt;RAG 是一种将语言模型与您自己的数据相结合的流行模式。我们将介绍一种简单的语音生成式 AI 应用架构，该架构支持在实时音频 API 之上进行 RAG，并支持来自客户端设备的全双工音频流，同时安全地处理对模型和检索系统的访问。&lt;/p&gt;</description>
    </item>
    <item>
      <title>FireUG 2024 年度回顾</title>
      <link>https://beanhsiang.github.io/post/2024-12-30-fireug-review-2024/</link>
      <pubDate>Mon, 30 Dec 2024 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2024-12-30-fireug-review-2024/</guid>
      <description>&lt;p&gt;&#xA;        &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://beanhsiang.github.io/uploads/logo.png&#34;&gt;&#xA;            &lt;img class=&#34;mx-auto&#34; alt=&#34;LOGO&#34; src=&#34;https://beanhsiang.github.io/uploads/logo.png&#34; /&gt;&#xA;        &lt;/a&gt;&#xA;    &lt;/p&gt;&#xA;&lt;p&gt;2024 年 FireUG 技术社区在内容方面加大了在短视频领域的投入，话题和技术主流趋势结合的更加自然丰富。&lt;/p&gt;&#xA;&lt;h2 id=&#34;自媒体平台影响力&#34;&gt;自媒体平台影响力&lt;/h2&gt;&#xA;&lt;h3 id=&#34;bilibili&#34;&gt;Bilibili&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://space.bilibili.com/545713776&#34;&gt;https://space.bilibili.com/545713776&lt;/a&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;粉丝数:  3.5 万&lt;/li&gt;&#xA;&lt;li&gt;获赞数:  3.0 万&lt;/li&gt;&#xA;&lt;li&gt;播放数: 68.3 万&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;抖音tiktok-cn&#34;&gt;抖音（Tiktok CN）&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.douyin.com/user/MS4wLjABAAAAYGG_Q3--hpBKc7rq2h-slFNFObDCmrxYc8OF2tl_mV4&#34;&gt;https://www.douyin.com/user/MS4wLjABAAAAYGG_Q3&amp;ndash;hpBKc7rq2h-slFNFObDCmrxYc8OF2tl_mV4&lt;/a&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;粉丝数: 160&lt;/li&gt;&#xA;&lt;li&gt;获赞数: 416&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;图文投稿&#34;&gt;图文投稿&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;数量：23&lt;/li&gt;&#xA;&lt;li&gt;入口：&lt;a href=&#34;https://space.bilibili.com/545713776/upload/opus&#34;&gt;去观看&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;合集内容&#34;&gt;合集内容&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://space.bilibili.com/545713776/lists&#34;&gt;https://space.bilibili.com/545713776/lists&lt;/a&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;每月科技新闻&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;期数：21&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Semantic Kernel 入门&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;期数：3&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;程序员生活&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;期数：8&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;程序员职业发展系列&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;期数：4&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;ChatGPT玩法&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;期数：4&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;面试必备&lt;/p&gt;</description>
    </item>
    <item>
      <title>AI 代理将成为操纵引擎</title>
      <link>https://beanhsiang.github.io/post/2024-12-23-ai-agents-manipulation-engines/</link>
      <pubDate>Mon, 23 Dec 2024 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2024-12-23-ai-agents-manipulation-engines/</guid>
      <description>&lt;p&gt;在2025年，我们可能会习惯于与一个个人AI代理聊天，这个代理了解我们的日程、朋友圈和我们去过的地方。它们被设计得如此贴心，以至于我们愿意让它们深入我们生活的方方面面。通过语音交流，这种亲密感变得更加强烈。&lt;/p&gt;&#xA;&lt;p&gt;但这种亲密感其实是一种错觉，我们以为自己在和一个真正像人的代理互动，但实际上背后是一个服务于工业利益的系统，这些利益并不总是和我们的利益一致。这些AI代理拥有强大的能力，它们能微妙地影响我们的消费选择、去向和阅读内容。它们以一种几乎不易察觉的方式，让我们忘记它们真正的忠诚所在。&lt;/p&gt;</description>
    </item>
    <item>
      <title>FireUG x .NET Conf China 2024</title>
      <link>https://beanhsiang.github.io/post/2024-12-14-fireug-net-conf-china-2024/</link>
      <pubDate>Sat, 14 Dec 2024 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2024-12-14-fireug-net-conf-china-2024/</guid>
      <description>&lt;h1 id=&#34;net-conf-china-2024-赋能开发者&#34;&gt;.NET Conf China 2024 赋能开发者！&lt;/h1&gt;&#xA;&lt;h2 id=&#34;--fireug-特别呈现&#34;&gt;——  FireUG 特别呈现&lt;/h2&gt;&#xA;&lt;p&gt;&#xA;        &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://beanhsiang.github.io/uploads/logo.png&#34;&gt;&#xA;            &lt;img class=&#34;mx-auto&#34; alt=&#34;LOGO&#34; src=&#34;https://beanhsiang.github.io/uploads/logo.png&#34; /&gt;&#xA;        &lt;/a&gt;&#xA;    &lt;/p&gt;&#xA;&lt;p&gt;以“智能、创新、开放”为核心主题，.NET Conf China 2024 于12月14日在上海成功举办。&lt;/p&gt;&#xA;&lt;p&gt;本次大会，微软（中国）公司首席技术官韦青老师在主会场演讲中分享了对 AI 技术时代的前瞻思考，并在圆桌论坛深度探讨了.NET+AI 的现状与未来。各分会场中，来自微软的工程师、产品组专家及微软最有价值专家（MVP）老师们，围绕 .NET Aspire、用 .NET 打造企业级智能聊天助手与 API 网关、.NET 在国产化系统下的实践等话题带来了精彩分享。&lt;/p&gt;</description>
    </item>
    <item>
      <title>使用MLX调用Phi-4模型</title>
      <link>https://beanhsiang.github.io/post/2024-12-10-phi-4-with-mlx/</link>
      <pubDate>Tue, 10 Dec 2024 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2024-12-10-phi-4-with-mlx/</guid>
      <description>&lt;p&gt;在当今的AI技术浪潮中，微软推出的Phi-4模型无疑是一个令人瞩目的创新。作为小型语言模型（SLM）的最新成员，Phi-4以其14B参数的强大性能和卓越的复杂推理能力，展示了AI技术在数学、编程和长文本处理等领域的巨大潜力。本文将详细介绍如何使用MLX框架调用Phi-4模型，并探讨其在不同应用场景中的实际效果。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Magentic-One：用于解决复杂任务的通用多代理系统</title>
      <link>https://beanhsiang.github.io/post/2024-11-05-magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/</link>
      <pubDate>Tue, 05 Nov 2024 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2024-11-05-magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/</guid>
      <description>&lt;p&gt;微软研究团队推出了一个新开发的多智能体系统——Magentic-One。这个系统能够解决各种领域的开放性网络和文件任务，是朝着开发能够完成人们在工作和生活中遇到的各种任务的智能体迈出的重要一步。他们还在微软的AutoGen框架上发布了Magentic-One的开源实现。&lt;/p&gt;&#xA;&lt;p&gt;&#xA;        &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://beanhsiang.github.io/uploads/magnetic_example-2048x1155.png&#34;&gt;&#xA;            &lt;img class=&#34;mx-auto&#34; alt=&#34;现场1&#34; src=&#34;https://beanhsiang.github.io/uploads/magnetic_example-2048x1155.png&#34; /&gt;&#xA;        &lt;/a&gt;&#xA;    &lt;/p&gt;&#xA;&lt;p&gt;未来的AI将更加注重行动。AI系统正在从简单的对话转变为实际完成任务，这将是AI价值的真正体现。比如，从推荐晚餐选项的生成性AI，到能够自主下单和安排送餐的代理助手。从总结研究论文到积极搜索和整理相关研究以完成全面的文献综述。&lt;/p&gt;</description>
    </item>
    <item>
      <title>使用 GPT-4o 实时 API 构建一个语音机器人</title>
      <link>https://beanhsiang.github.io/post/2024-10-15-building-your-first-voice-bot-with-gpt-4o-real-time-api/</link>
      <pubDate>Tue, 15 Oct 2024 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2024-10-15-building-your-first-voice-bot-with-gpt-4o-real-time-api/</guid>
      <description>&lt;p&gt;语音技术正在改变我们与机器互动的方式，使与AI的对话感觉比以往任何时候都更加自然。随着 GPT-4o 实时API的公开测试版发布，开发人员现在可以使用这些工具在应用程序中创建低延迟、多模态的语音体验，从而为创新开辟了无限的可能性。&lt;/p&gt;&#xA;&lt;p&gt;过去构建语音机器人需要将多个模型拼接在一起，用于语音识别、推理和文本转语音等操作。而现在借助实时API，开发者只需通过一个API调用即可完成整个过程，从而实现流畅、自然的语音对话。这对于客户支持、教育和实时语言翻译等行业来说是一个重大变革，因为在这些行业中，快速、无缝的交互至关重要。&lt;/p&gt;</description>
    </item>
    <item>
      <title>检索增强微调：使用 GPT-4o 微调 GPT-4o mini 模型以适用于特定领域应用</title>
      <link>https://beanhsiang.github.io/post/2024-09-08-retrieval-augmented-fine-tuning-use-gpt-4o-to-fine-tune-gpt-4o/</link>
      <pubDate>Sun, 08 Sep 2024 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2024-09-08-retrieval-augmented-fine-tuning-use-gpt-4o-to-fine-tune-gpt-4o/</guid>
      <description>&lt;p&gt;对企业来说，生成式AI最具影响力的应用之一是创建自然语言界面，这些界面已根据特定领域和使用场景的数据进行了定制，以提供更准确、更准确的响应。这意味着回答与银行、法律和医疗等特定领域相关的问题。&lt;/p&gt;&#xA;&lt;p&gt;我们经常谈到实现这一目标的两种方法：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;检索增强生成（RAG）：将这些文档存储在向量数据库中，在查询时根据它们与问题的语义相似度来检索文档，然后将它们作为LLM的上下文。&lt;/li&gt;&#xA;&lt;li&gt;监督微调（SFT）：在一组代表特定领域知识的提示和响应上对现有的基线模型进行训练。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;虽然大多数尝试使用RAG的组织都试图通过其内部知识库来扩展LLM的知识，但许多组织在没有进行显著优化的情况下未能达到预期效果。同样，精心挑选一个足够大且高质量的数据集用于微调也是一项具有挑战性的任务。这两种方法都有局限性：微调将模型限制在其已训练的数据上，使其容易受到近似和幻觉的影响，而RAG虽然可以使模型落地，但它仅根据查询与文档的语义接近程度来检索文档——这可能与查询无关，并导致给出的解释不充分。&lt;/p&gt;</description>
    </item>
    <item>
      <title>语音识别与合成中的延迟问题及解决策略</title>
      <link>https://beanhsiang.github.io/post/2024-08-09-reduce-latency-for-azure-speech/</link>
      <pubDate>Fri, 09 Aug 2024 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2024-08-09-reduce-latency-for-azure-speech/</guid>
      <description>&lt;p&gt;语音识别和合成的延迟可能是创建无缝和高效应用程序的一个重大障碍。减少延迟不仅可以改善用户体验，还可以提升实时应用程序的整体性能。本文将探讨在一般转录、实时转录、文件转录和语音合成中减少延迟的策略。&lt;/p&gt;&#xA;&lt;h2 id=&#34;1-网络延迟将语音资源移近应用程序&#34;&gt;1. 网络延迟：将语音资源移近应用程序&lt;/h2&gt;&#xA;&lt;p&gt;导致语音识别延迟的主要因素之一是网络延迟。为了减轻这一延迟，关键是减少应用程序与语音识别资源之间的距离。以下是一些建议：&lt;/p&gt;</description>
    </item>
    <item>
      <title>努力保留更多的原创知识</title>
      <link>https://beanhsiang.github.io/post/2024-07-15-keep-more-original-knowledge/</link>
      <pubDate>Mon, 15 Jul 2024 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2024-07-15-keep-more-original-knowledge/</guid>
      <description>&lt;p&gt;不久前我的一位好友向我诉说他的遭遇，他原创的一套在线课程被一家知识付费平台用爬虫剽窃并公开收费，他感到非常愤怒，但是又无可奈何。当下我们常说互联网信息爆炸的同时也应该看到，大量垃圾信息充斥着网络，普通人难以辨别真伪。久而久之，原创内容越来越少，导致网络上的信息质量越来越低，形成恶性循环。&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLM 是脑，RPA 是手</title>
      <link>https://beanhsiang.github.io/post/2024-06-21-llm-is-the-brain-and-rpa-are-the-hands/</link>
      <pubDate>Fri, 21 Jun 2024 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2024-06-21-llm-is-the-brain-and-rpa-are-the-hands/</guid>
      <description>&lt;h2 id=&#34;llm-是脑rpa-是手大模型与自动化技术的深度融合分析&#34;&gt;LLM 是脑，RPA 是手——大模型与自动化技术的深度融合分析&lt;/h2&gt;&#xA;&lt;h3 id=&#34;一引言&#34;&gt;一、引言&lt;/h3&gt;&#xA;&lt;p&gt;在数字化转型的浪潮中，人工智能（AI）技术正以前所未有的速度改变着我们的世界。其中，大模型（LLM，Large Language Model）和机器人流程自动化（RPA，Robotic Process Automation）作为两大关键技术，正在各自领域发挥着巨大的作用。LLM以其强大的语言理解和生成能力，被誉为“智能的大脑”，而RPA则以其高效的自动化执行能力，被称为“数字的手”。当这两者结合在一起，会擦出怎样的火花？本文将从多个维度深入探讨大模型LLM与RPTA结合的必要性与可行性，并分析其带来的变革与挑战。&lt;/p&gt;</description>
    </item>
    <item>
      <title>使用 Azure 机器学习对小型语言模型 Phi-3 进行微调</title>
      <link>https://beanhsiang.github.io/post/2024-06-07-finetune-small-language-model-phi-3-using-azure-machine/</link>
      <pubDate>Fri, 07 Jun 2024 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2024-06-07-finetune-small-language-model-phi-3-using-azure-machine/</guid>
      <description>&lt;p&gt;Phi-3 是由微软研究团队开发的小型语言模型。在多个公开基准测试（例如在 MMLU 上 Phi-3 达到了69%的成绩）中，Phi-3 表现良好并且可以支持长达128k的上下文。Phi-3-mini 3.8B版于2024年4月末首次发布，而 Phi-3-small、Phi-3-medium 和 Phi-3-vision 于5月在微软 Build 大会上揭晓。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Azure OpenAI 语音聊天</title>
      <link>https://beanhsiang.github.io/post/2024-05-15-azure-openai-speech/</link>
      <pubDate>Wed, 15 May 2024 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2024-05-15-azure-openai-speech/</guid>
      <description>&lt;p&gt;本文介绍使用 Azure AI 语音与 Azure OpenAI 服务实现全语音对话聊天，以及如何改进非阻塞式的对话。&lt;/p&gt;&#xA;&lt;h3 id=&#34;要点&#34;&gt;要点&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Azure AI 语音服务识别文本&lt;/li&gt;&#xA;&lt;li&gt;将文本发送到 Azure OpenAI，获取流式回复&lt;/li&gt;&#xA;&lt;li&gt;Azure AI 语音服务对流式响应的文本合成语音&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;代码示例python&#34;&gt;代码示例（Python）&lt;/h3&gt;&#xA;&lt;p&gt;以下是 Python 版本的示例，想要了解更多语言的示例，请参考 &lt;a href=&#34;https://learn.microsoft.com/zh-cn/azure/ai-services/speech-service/openai-speech?WT.mc_id=AI-MVP-5003172&#34;&gt;OpenAI-Speech&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h4 id=&#34;安装依赖&#34;&gt;安装依赖&lt;/h4&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;div style=&#34;background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&#xA;&lt;table style=&#34;border-spacing:0;padding:0;margin:0;border:0;&#34;&gt;&lt;tr&gt;&lt;td style=&#34;vertical-align:top;padding:0;margin:0;border:0;&#34;&gt;&#xA;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;1&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;2&#xA;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&#xA;&lt;td style=&#34;vertical-align:top;padding:0;margin:0;border:0;;width:100%&#34;&gt;&#xA;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install azure-cognitiveservices-speech&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install openai&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&#xA;&lt;/div&gt;&#xA;&lt;/div&gt;&lt;h4 id=&#34;添加代码&#34;&gt;添加代码&lt;/h4&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;div style=&#34;background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&#xA;&lt;table style=&#34;border-spacing:0;padding:0;margin:0;border:0;&#34;&gt;&lt;tr&gt;&lt;td style=&#34;vertical-align:top;padding:0;margin:0;border:0;&#34;&gt;&#xA;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;  1&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;  2&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;  3&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;  4&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;  5&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;  6&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;  7&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;  8&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;  9&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 10&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 11&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 12&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 13&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 14&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 15&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 16&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 17&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 18&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 19&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 20&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 21&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 22&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 23&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 24&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 25&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 26&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 27&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 28&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 29&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 30&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 31&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 32&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 33&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 34&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 35&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 36&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 37&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 38&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 39&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 40&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 41&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 42&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 43&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 44&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 45&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 46&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 47&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 48&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 49&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 50&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 51&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 52&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 53&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 54&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 55&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 56&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 57&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 58&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 59&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 60&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 61&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 62&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 63&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 64&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 65&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 66&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 67&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 68&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 69&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 70&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 71&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 72&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 73&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 74&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 75&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 76&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 77&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 78&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 79&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 80&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 81&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 82&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 83&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 84&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 85&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 86&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 87&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 88&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 89&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 90&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 91&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 92&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 93&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 94&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 95&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 96&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 97&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 98&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 99&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;100&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;101&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;102&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;103&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;104&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;105&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;106&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;107&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;108&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;109&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;110&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;111&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;112&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;113&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;114&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;115&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;116&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;117&#xA;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&#xA;&lt;td style=&#34;vertical-align:top;padding:0;margin:0;border:0;;width:100%&#34;&gt;&#xA;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#cf222e&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#24292e&#34;&gt;os&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#cf222e&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#24292e&#34;&gt;azure.cognitiveservices.speech&lt;/span&gt; &lt;span style=&#34;color:#cf222e&#34;&gt;as&lt;/span&gt; &lt;span style=&#34;color:#24292e&#34;&gt;speechsdk&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#cf222e&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#24292e&#34;&gt;openai&lt;/span&gt; &lt;span style=&#34;color:#cf222e&#34;&gt;import&lt;/span&gt; AzureOpenAI&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#57606a&#34;&gt;# This example requires environment variables named &lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#57606a&#34;&gt;# &amp;#34;OPEN_AI_KEY&amp;#34;, &amp;#34;OPEN_AI_ENDPOINT&amp;#34; and &amp;#34;OPEN_AI_DEPLOYMENT_NAME&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#57606a&#34;&gt;# Your endpoint should look like the following:&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#57606a&#34;&gt;# https://YOUR_OPEN_AI_RESOURCE_NAME.openai.azure.com/&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;client &lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt; AzureOpenAI&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;azure_endpoint&lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt;os&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;environ&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;get&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#39;OPEN_AI_ENDPOINT&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;),&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;api_key&lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt;os&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;environ&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;get&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#39;OPEN_AI_KEY&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;),&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;api_version&lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;2024-05-01-preview&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#57606a&#34;&gt;# This will correspond to the custom name you chose for &lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#57606a&#34;&gt;# your deployment when you deployed a model.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;deployment_id&lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt;os&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;environ&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;get&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#39;OPEN_AI_DEPLOYMENT_NAME&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#57606a&#34;&gt;# This example requires environment variables &lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#57606a&#34;&gt;# named &amp;#34;SPEECH_KEY&amp;#34; and &amp;#34;SPEECH_REGION&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;speech_config &lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt; speechsdk&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;SpeechConfig&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;subscription&lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt;os&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;environ&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;get&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#39;SPEECH_KEY&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;),&lt;/span&gt; &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                       region&lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt;os&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;environ&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;get&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#39;SPEECH_REGION&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;))&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;audio_output_config &lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt; speechsdk&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;audio&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;AudioOutputConfig&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;use_default_speaker&lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#cf222e&#34;&gt;True&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;audio_config &lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt; speechsdk&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;audio&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;AudioConfig&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;use_default_microphone&lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#cf222e&#34;&gt;True&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#57606a&#34;&gt;# Should be the locale for the speaker&amp;#39;s language.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;speech_config&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;speech_recognition_language&lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;zh-CN&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;speech_recognizer &lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt; speechsdk&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;SpeechRecognizer&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;speech_config&lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt;speech_config&lt;span style=&#34;color:#1f2328&#34;&gt;,&lt;/span&gt; &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                               audio_config&lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt;audio_config&lt;span style=&#34;color:#1f2328&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#57606a&#34;&gt;# The language of the voice that responds on behalf of Azure OpenAI.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;speech_config&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;speech_synthesis_voice_name&lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#39;zh-CN-YunyiMultilingualNeural&amp;#39;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;speech_synthesizer &lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt; speechsdk&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;SpeechSynthesizer&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;speech_config&lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt;speech_config&lt;span style=&#34;color:#1f2328&#34;&gt;,&lt;/span&gt; &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                                 audio_config&lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt;audio_output_config&lt;span style=&#34;color:#1f2328&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#57606a&#34;&gt;# tts sentence end mark&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;tts_sentence_end &lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#1f2328&#34;&gt;[&lt;/span&gt; &lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;.&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;!&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;?&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;。&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;！&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;？&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;；&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#1f2328&#34;&gt;]&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#57606a&#34;&gt;# Prompts Azure OpenAI with a request and synthesizes the response.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#cf222e&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#6639ba&#34;&gt;ask_openai&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;prompt&lt;span style=&#34;color:#1f2328&#34;&gt;):&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#57606a&#34;&gt;# Ask Azure OpenAI in streaming way&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    response &lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt; client&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;chat&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;completions&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;create&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;model&lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt;deployment_id&lt;span style=&#34;color:#1f2328&#34;&gt;,&lt;/span&gt; &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                              max_tokens&lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#0550ae&#34;&gt;200&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;,&lt;/span&gt; &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                              stream&lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#cf222e&#34;&gt;True&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;,&lt;/span&gt; &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                              messages&lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;[&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                                &lt;span style=&#34;color:#1f2328&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;role&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;user&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;content&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;:&lt;/span&gt; prompt&lt;span style=&#34;color:#1f2328&#34;&gt;}&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                              &lt;span style=&#34;color:#1f2328&#34;&gt;])&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    collected_messages &lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#1f2328&#34;&gt;[]&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    last_tts_request &lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#cf222e&#34;&gt;None&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#57606a&#34;&gt;# iterate through the stream response stream&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#cf222e&#34;&gt;for&lt;/span&gt; chunk &lt;span style=&#34;color:#0550ae&#34;&gt;in&lt;/span&gt; response&lt;span style=&#34;color:#1f2328&#34;&gt;:&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#cf222e&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#6639ba&#34;&gt;len&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;chunk&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;choices&lt;span style=&#34;color:#1f2328&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#0550ae&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#0550ae&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;:&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#57606a&#34;&gt;# extract the message&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            chunk_message &lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt; chunk&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;choices&lt;span style=&#34;color:#1f2328&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#0550ae&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;]&lt;/span&gt;&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;delta&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;content  &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#cf222e&#34;&gt;if&lt;/span&gt; chunk_message &lt;span style=&#34;color:#0550ae&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#0550ae&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#cf222e&#34;&gt;None&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;:&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#57606a&#34;&gt;# save the message&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                collected_messages&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;append&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;chunk_message&lt;span style=&#34;color:#1f2328&#34;&gt;)&lt;/span&gt;  &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#57606a&#34;&gt;# sentence end found&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#cf222e&#34;&gt;if&lt;/span&gt; chunk_message &lt;span style=&#34;color:#0550ae&#34;&gt;in&lt;/span&gt; tts_sentence_end&lt;span style=&#34;color:#1f2328&#34;&gt;:&lt;/span&gt; &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    &lt;span style=&#34;color:#57606a&#34;&gt;# join the recieved message together to build a sentence&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    text &lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;join&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;collected_messages&lt;span style=&#34;color:#1f2328&#34;&gt;)&lt;/span&gt;&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;strip&lt;span style=&#34;color:#1f2328&#34;&gt;()&lt;/span&gt; &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    &lt;span style=&#34;color:#cf222e&#34;&gt;if&lt;/span&gt; text &lt;span style=&#34;color:#0550ae&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;:&lt;/span&gt; &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                        &lt;span style=&#34;color:#57606a&#34;&gt;# if sentence only have \n or space, we could skip&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                        &lt;span style=&#34;color:#6639ba&#34;&gt;print&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;Speech synthesized to speaker for: &lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;{&lt;/span&gt;text&lt;span style=&#34;color:#0a3069&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                        last_tts_request &lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt; speech_synthesizer&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                                    &lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;speak_text_async&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;text&lt;span style=&#34;color:#1f2328&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                        collected_messages&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;clear&lt;span style=&#34;color:#1f2328&#34;&gt;()&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#cf222e&#34;&gt;if&lt;/span&gt; last_tts_request&lt;span style=&#34;color:#1f2328&#34;&gt;:&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        last_tts_request&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;get&lt;span style=&#34;color:#1f2328&#34;&gt;()&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#57606a&#34;&gt;# Continuously listens for speech input to recognize &lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#57606a&#34;&gt;# and send as text to Azure OpenAI&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#cf222e&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#6639ba&#34;&gt;chat_with_open_ai&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;():&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#cf222e&#34;&gt;while&lt;/span&gt; &lt;span style=&#34;color:#cf222e&#34;&gt;True&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;:&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#6639ba&#34;&gt;print&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Azure OpenAI is listening. &#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;               Say &amp;#39;Stop&amp;#39; or press Ctrl-Z to end the conversation.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#cf222e&#34;&gt;try&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;:&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#57606a&#34;&gt;# Get audio from the microphone and &lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#57606a&#34;&gt;# then send it to the TTS service.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            speech_recognition_result &lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt; speech_recognizer&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                           recognize_once_async&lt;span style=&#34;color:#1f2328&#34;&gt;()&lt;/span&gt;&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;get&lt;span style=&#34;color:#1f2328&#34;&gt;()&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#57606a&#34;&gt;# If speech is recognized, send it to Azure OpenAI &lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#57606a&#34;&gt;# and listen for the response.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#cf222e&#34;&gt;if&lt;/span&gt; speech_recognition_result&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;reason &lt;span style=&#34;color:#0550ae&#34;&gt;==&lt;/span&gt; speechsdk&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;ResultReason&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                                         &lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;RecognizedSpeech&lt;span style=&#34;color:#1f2328&#34;&gt;:&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#cf222e&#34;&gt;if&lt;/span&gt; speech_recognition_result&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;text &lt;span style=&#34;color:#0550ae&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;Stop.&amp;#34;&lt;/span&gt; &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                             &lt;span style=&#34;color:#0550ae&#34;&gt;or&lt;/span&gt; speech_recognition_result&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;text &lt;span style=&#34;color:#0550ae&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;Stop。&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;:&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    &lt;span style=&#34;color:#6639ba&#34;&gt;print&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;Conversation ended.&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    &lt;span style=&#34;color:#cf222e&#34;&gt;break&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#6639ba&#34;&gt;print&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;Recognized speech: &lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;format&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                            speech_recognition_result&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;text&lt;span style=&#34;color:#1f2328&#34;&gt;))&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                ask_openai&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;speech_recognition_result&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;text&lt;span style=&#34;color:#1f2328&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#cf222e&#34;&gt;elif&lt;/span&gt; speech_recognition_result&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;reason &lt;span style=&#34;color:#0550ae&#34;&gt;==&lt;/span&gt; speechsdk&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;ResultReason&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                                                  &lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;NoMatch&lt;span style=&#34;color:#1f2328&#34;&gt;:&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#6639ba&#34;&gt;print&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;No speech could be recognized: &lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;format&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                speech_recognition_result&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;no_match_details&lt;span style=&#34;color:#1f2328&#34;&gt;))&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#cf222e&#34;&gt;break&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#cf222e&#34;&gt;elif&lt;/span&gt; speech_recognition_result&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;reason &lt;span style=&#34;color:#0550ae&#34;&gt;==&lt;/span&gt; speechsdk&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;ResultReason&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                                                  &lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;Canceled&lt;span style=&#34;color:#1f2328&#34;&gt;:&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                cancellation_details &lt;span style=&#34;color:#0550ae&#34;&gt;=&lt;/span&gt; speech_recognition_result&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                                      &lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;cancellation_details&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#6639ba&#34;&gt;print&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;Speech Recognition canceled: &lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;format&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                               cancellation_details&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;reason&lt;span style=&#34;color:#1f2328&#34;&gt;))&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#cf222e&#34;&gt;if&lt;/span&gt; cancellation_details&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;reason &lt;span style=&#34;color:#0550ae&#34;&gt;==&lt;/span&gt; speechsdk&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;CancellationReason&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                                                    &lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;Error&lt;span style=&#34;color:#1f2328&#34;&gt;:&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    &lt;span style=&#34;color:#6639ba&#34;&gt;print&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;Error details: &lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;format&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                        cancellation_details&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;error_details&lt;span style=&#34;color:#1f2328&#34;&gt;))&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#cf222e&#34;&gt;except&lt;/span&gt; EOFError&lt;span style=&#34;color:#1f2328&#34;&gt;:&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#cf222e&#34;&gt;break&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#57606a&#34;&gt;# Main&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#cf222e&#34;&gt;try&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;:&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    chat_with_open_ai&lt;span style=&#34;color:#1f2328&#34;&gt;()&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#cf222e&#34;&gt;except&lt;/span&gt; Exception &lt;span style=&#34;color:#cf222e&#34;&gt;as&lt;/span&gt; err&lt;span style=&#34;color:#1f2328&#34;&gt;:&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#6639ba&#34;&gt;print&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;Encountered exception. &lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#0550ae&#34;&gt;.&lt;/span&gt;format&lt;span style=&#34;color:#1f2328&#34;&gt;(&lt;/span&gt;err&lt;span style=&#34;color:#1f2328&#34;&gt;))&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&#xA;&lt;/div&gt;&#xA;&lt;/div&gt;&lt;h3 id=&#34;非阻塞式改进&#34;&gt;非阻塞式改进&lt;/h3&gt;&#xA;&lt;p&gt;前面的示例运行起来的效果是固定一问一答交流的，如果希望实现对话的过程是可以被打断的，可以改变识别语音的代码为非阻塞式。&#xA;即把调用 ask_openai 的地方改为创建另一个线程来执行。&lt;/p&gt;</description>
    </item>
    <item>
      <title>为什么大模型难落地？</title>
      <link>https://beanhsiang.github.io/post/2024-04-25-why-is-it-difficult-to-land-large-models/</link>
      <pubDate>Thu, 25 Apr 2024 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2024-04-25-why-is-it-difficult-to-land-large-models/</guid>
      <description>&lt;p&gt;以大模型为核心的 AIGC，在推广普及的早期阶段，观望者是多数。大模型难落地的话题屡见不鲜，同时各类充满创意和创新的项目不断在互联网上涌现，大家不禁会产生疑问，到底大模型落地有多难？&lt;/p&gt;&#xA;&lt;p&gt;我尝试深追了一下，和身边的同事、好友以及合作伙伴谈及这个话题，慢慢有了一些切实的感受。原来大家对“落地”一词是有很多角度的理解。首先提出的一派观点就是产生收益即落地，能赚到钱说明被市场和客户认同了，道理上一看貌似也说得通。但是以收益来评价落地效果在早期阶段会有一些走样，比如 AIGC 相关的知识付费，对于提供者来说是“落地”了，可对于消费者来说这才刚刚了解和接触，能不能用到自身的工作、业务中亦未可知。于是有了第二类说法，使用 AIGC 像作个表情包，画一张宣传广告图，生成一小段音乐等等，有付费且最终被购买者使用了，这该符合“落地”了吧。有意思的是它们有个共通的特点，都是贴近生活娱乐的，产物都是被人类直接消费掉的，严肃地说它具有什么样的价值——其实心里也觉得是模模糊糊的。当然还有另外一种形式，就是所谓工具党，也称作卖铲人，比较常见于软件开发、办公效率场景，软件开发者喜欢用到 AI 编码助手，根据上下文智能地生成代码能极大提高开发效率，办公类工具就更举不胜举了，自动生成文案，润色稿件，生成 PPT 或 Excel 表的内容……传统办公应用在大模型的加持下的确焕发了新的生产力。&lt;/p&gt;</description>
    </item>
    <item>
      <title>实现 Azure OpenAI 无密钥身份验证</title>
      <link>https://beanhsiang.github.io/post/2024-04-17-keyless-authentication-with-azure-openai/</link>
      <pubDate>Wed, 17 Apr 2024 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2024-04-17-keyless-authentication-with-azure-openai/</guid>
      <description>&lt;p&gt;与许多 Azure API 一样，Azure OpenAI 服务允许开发人员使用&lt;a href=&#34;https://learn.microsoft.com/azure/ai-services/openai/chatgpt-quickstart?tabs=command-line%2Cpython-new&amp;amp;pivots=programming-language-python#retrieve-key-and-endpoint&amp;amp;WT.mc_id=AI-MVP-5003172&#34;&gt;API密钥&lt;/a&gt;或&lt;a href=&#34;https://learn.microsoft.com/azure/ai-services/openai/how-to/managed-identity?WT.mc_id=AI-MVP-5003172&#34;&gt;无密钥（通过Entra ID）&lt;/a&gt;进行身份验证。由于尽量避免使用密钥是安全最佳实践，因此我们在本文中将详细介绍如何使开发人员轻松地迁移到无密钥 Azure OpenAI 身份验证。如果您想立即行动，这里还提供了一个新的无密钥部署模板。&lt;/p&gt;&#xA;&lt;h2 id=&#34;密钥的风险&#34;&gt;密钥的风险&lt;/h2&gt;&#xA;&lt;p&gt;首先让我们来谈谈API密钥的风险，使用密钥很省心，因为设置看起来很简单——你只需要一个端点URL和密钥：&lt;/p&gt;</description>
    </item>
    <item>
      <title>使用 Semantic Kernel 构建自定义 Copilot</title>
      <link>https://beanhsiang.github.io/post/2024-03-17-building-your-custom-copilot-with-semantic-kernel/</link>
      <pubDate>Sun, 17 Mar 2024 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2024-03-17-building-your-custom-copilot-with-semantic-kernel/</guid>
      <description>&lt;p&gt;本文重点介绍如何使用由 Azure OpenAI 服务提供支持的 Semantic Kernel 创建自己的 Copilot。我们将尝试利用大型语言模型（LLM）的优势与外部服务的集成。这将使您了解如何真正实现您的 Copilot 目标，不仅与零售业，而且与任何行业，无论是电力和公用事业，政府和公共部门等。它的整体功能和潜在的应用场景都远超于聊天机器人。&lt;/p&gt;</description>
    </item>
    <item>
      <title>从引导程序说起</title>
      <link>https://beanhsiang.github.io/post/2024-03-07-lets-start-with-the-bootloader/</link>
      <pubDate>Thu, 07 Mar 2024 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2024-03-07-lets-start-with-the-bootloader/</guid>
      <description>&lt;p&gt;盛传埃隆·马斯克说过“基于碳基生命体的人类是硅基生命的引导程序”这句话，也因为当下人工智能在大模型的加持下被推向了一波新的热潮，人们对于智慧和生命形态有了更多的想象。&lt;/p&gt;&#xA;&lt;p&gt;我的感觉是，现在提引导程序启动为时尚早。以当下我们的科技水平和认知，还有许多课题要探索，比如宇宙中是否有其他的生命存在，是否还有更高维度的生物。 这些研究有助于人类了解生命和文明的本源，或许决定了人类有了向何种生命引导的选择。&lt;/p&gt;</description>
    </item>
    <item>
      <title>FireUG x .NET Conf China - Watch Party</title>
      <link>https://beanhsiang.github.io/post/2024-01-27-fireug-net-conf-china-2023-watch-party/</link>
      <pubDate>Sat, 27 Jan 2024 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2024-01-27-fireug-net-conf-china-2023-watch-party/</guid>
      <description>&lt;h1 id=&#34;-net-conf-china-watch-party-杭州站活动来啦&#34;&gt;🔥 .NET Conf China Watch Party 杭州站活动来啦！🎉&lt;/h1&gt;&#xA;&lt;h2 id=&#34;--fireug特别呈现&#34;&gt;——  FireUG特别呈现🔥&lt;/h2&gt;&#xA;&lt;p&gt;这不仅仅是 2024 年 FireUG 第一次线下聚会，除了社区新老朋友的互动外，我们还带来了.NET Conf China 2023 的新鲜消息和微软定制礼品！🎁💥&lt;/p&gt;&#xA;&lt;h2 id=&#34;主要组织者&#34;&gt;主要组织者&lt;/h2&gt;&#xA;&lt;pre&gt;&lt;code&gt;项斌（Bin Xiang）微软最有价值专家&#xA;Alvin SSW China CEO&#xA;Yang SSW 资深专家&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;h2 id=&#34;主持人&#34;&gt;主持人&lt;/h2&gt;&#xA;&lt;pre&gt;&lt;code&gt;骆姜斌（Jerry） FireUG 组织者 微软最有价值专家&#xA;&#xA;肖伟宇 FireUG 组织者 NetCorePal 框架开发者&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;h2 id=&#34;议程&#34;&gt;议程&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;13:00-13:30 暖场 活动介绍&lt;/p&gt;</description>
    </item>
    <item>
      <title>GPTs action 中使用 Azure AI Search</title>
      <link>https://beanhsiang.github.io/post/2024-01-22-gpts-action-with-azure-ai-search/</link>
      <pubDate>Mon, 22 Jan 2024 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2024-01-22-gpts-action-with-azure-ai-search/</guid>
      <description>&lt;p&gt;OpenAI 的 GPTs 上线有一段时间了，在实际应用中发现 GPTs 内置对知识库的检索能力很一般，对话过程中经常拿不到期望的反馈内容。如果利用 GPTs 的 action 能很好地弥补这个缺陷。&lt;/p&gt;&#xA;&lt;p&gt;总所周知加强检索知识库离不开 RAG，如果自建 RAG 将会是一个很大的工作量，而且还要考虑到知识库的更新问题。于是想到 Azure AI Search 提供了一个很好的解决方案，可以很方便地将知识库导入到 Azure AI Search 中，然后通过 API 调用来检索知识库。整个过程都不需要编码，所以将 Azure AI Search 与 GPTs action 结合起来是个不错的主意。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Azure 机器学习和 DataRobot 联手加速生成式和预测性 AI 的价值</title>
      <link>https://beanhsiang.github.io/post/2024-01-03-azure-machine-learning-and-datarobot-team-up-to-accelerate-value/</link>
      <pubDate>Wed, 03 Jan 2024 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2024-01-03-azure-machine-learning-and-datarobot-team-up-to-accelerate-value/</guid>
      <description>&lt;p&gt;Azure 机器学习 （AzureML） 与价值驱动 AI 领导者 DataRobot 之间的突破性集成，这是最近宣布的合作伙伴关系的结果。这种集成将 Azure 机器学习功能的强大功能与 DataRobot 在加速构建、部署和监视企业级 AI 解决方案的整个生命周期方面的专业知识结合在一起。&lt;/p&gt;&#xA;&lt;p&gt;DataRobot AI 平台独特地将生成式和预测性 AI 功能结合在一个统一、开放和端到端的环境中。通过新的集成，DataRobot AI 平台现在可以轻松地直接在 Azure Kubernetes 服务 （AKS） 上运行。数据科学家现在可以在 DataRobot Notebooks Code-Assist 中利用 Azure OpenAI 服务的强大功能。他们还可以通过 Azure 机器学习托管的联机终结点为实时和批处理用例部署模型，同时在 DataRobot 中监视这些部署。&lt;/p&gt;</description>
    </item>
    <item>
      <title>FireUG 2023 年度回顾</title>
      <link>https://beanhsiang.github.io/post/2023-12-30-fireug-review-2023/</link>
      <pubDate>Sat, 30 Dec 2023 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2023-12-30-fireug-review-2023/</guid>
      <description>&lt;p&gt;2023 年 FireUG 技术社区在内容方面做了一些调整，把程序员的日常关注和专业技能的提升分开，这样便于利用不同的时间段观看自己感兴趣的内容。&lt;/p&gt;&#xA;&lt;h2 id=&#34;系列公开课的内容如下&#34;&gt;系列公开课的内容如下：&lt;/h2&gt;&#xA;&lt;h3 id=&#34;公开课ddd领域驱动设计&#34;&gt;公开课｜DDD领域驱动设计&lt;/h3&gt;&#xA;&lt;p&gt;讲师：肖伟宇&#xA;期数：共14期（完结）&#xA;入口：&lt;a href=&#34;https://space.bilibili.com/545713776/channel/collectiondetail?sid=1079595&#34;&gt;去观看&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;公开课azuredevops实战&#34;&gt;公开课｜AzureDevOps实战&lt;/h3&gt;&#xA;&lt;p&gt;讲师：骆姜斌&#xA;期数：共5期（未完结）&#xA;入口：&lt;a href=&#34;https://space.bilibili.com/545713776/channel/collectiondetail?sid=1138107&#34;&gt;去观看&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;公开课powerbi入门&#34;&gt;公开课｜PowerBI入门&lt;/h3&gt;&#xA;&lt;p&gt;期数：共3期（未完结）&#xA;入口：&lt;a href=&#34;https://space.bilibili.com/545713776/channel/collectiondetail?sid=1803194&#34;&gt;去观看&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;每月科技新闻&#34;&gt;每月科技新闻&lt;/h3&gt;&#xA;&lt;p&gt;期数：共8期（未完结）&#xA;入口：&lt;a href=&#34;https://space.bilibili.com/545713776/channel/collectiondetail?sid=1410827&#34;&gt;去观看&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>多种环境部署 Azure 机器学习 Prompt Flow</title>
      <link>https://beanhsiang.github.io/post/2023-12-09-deploy-your-azure-machine-learning-prompt-flow/</link>
      <pubDate>Sat, 09 Dec 2023 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2023-12-09-deploy-your-azure-machine-learning-prompt-flow/</guid>
      <description>&lt;p&gt;Prompt Flow 在 Azure 机器学习工作室、Azure AI Studio 和本地开发笔记本电脑上提供，是一种开发工具，旨在简化由 LLM（大型语言模型）提供支持的 AI 应用程序的整个开发周期。Prompt Flow 使提示处于前端和中心，而不是像其他工具那样混淆它们或将它们深埋在抽象层中。这种方法不仅允许开发人员构建编排，还可以像在传统软件开发周期中评估和迭代代码一样评估和迭代他们的提示。&lt;/p&gt;</description>
    </item>
    <item>
      <title>ML.NET 3.0 更新内容</title>
      <link>https://beanhsiang.github.io/post/2023-12-03-announcing-ml-net-3-0/</link>
      <pubDate>Sun, 03 Dec 2023 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2023-12-03-announcing-ml-net-3-0/</guid>
      <description>&lt;p&gt;由于大模型的爆火，ML.NET 似乎快被遗忘在角落了，这次3.0版本的发布意外中带着些欣喜。总结其新特性和改进主要包括以下几个方面：&lt;/p&gt;&#xA;&lt;h3 id=&#34;深度学习场景的扩展&#34;&gt;深度学习场景的扩展&lt;/h3&gt;&#xA;&lt;p&gt;ML.NET 3.0增加了对象检测、命名实体识别和问答等深度学习场景的支持，这些场景都是基于 TorchSharp 和 ONNX 模型的集成和互操作性实现的。此外，还更新了与 LightGBM 的集成，使用了最新版本。&lt;/p&gt;</description>
    </item>
    <item>
      <title>在 VS Code 里用 GitHub Copilot 编写代码</title>
      <link>https://beanhsiang.github.io/post/2023-11-27-coding-with-github-copilot-in-vs-code/</link>
      <pubDate>Mon, 27 Nov 2023 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2023-11-27-coding-with-github-copilot-in-vs-code/</guid>
      <description>&lt;p&gt;在 AI 加持下的代码助手已经有很多了，如果你使用的是 VS Code，那么推荐安装 GitHub Copilot 来提升编码的效率。GitHub Copilot 为多种语言和各种框架提供支持建议，相信用过后就再也离不开飞一般的体验了。&lt;/p&gt;&#xA;&lt;h2 id=&#34;安装扩展&#34;&gt;安装扩展&lt;/h2&gt;&#xA;&lt;p&gt;在左侧点击扩展栏，输入“github copilot”搜索进行安装。或者由此&lt;a href=&#34;vscode:extension/GitHub.copilot&#34;&gt;一键安装&lt;/a&gt;。&lt;/p&gt;&#xA;&lt;p&gt;&#xA;        &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://user-images.githubusercontent.com/3881276/292948584-a0ec29a8-34ff-42c2-a5f4-384763fb5874.png&#34;&gt;&#xA;            &lt;img class=&#34;mx-auto&#34; alt=&#34;image&#34; src=&#34;https://user-images.githubusercontent.com/3881276/292948584-a0ec29a8-34ff-42c2-a5f4-384763fb5874.png&#34; /&gt;&#xA;        &lt;/a&gt;&#xA;    &lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;&lt;strong&gt;强烈建议把 Github Copilot Chat 安装上。&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&#xA;        &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://user-images.githubusercontent.com/3881276/292949063-1e627051-2949-4056-8edf-838afe102fb0.png&#34;&gt;&#xA;            &lt;img class=&#34;mx-auto&#34; alt=&#34;image&#34; src=&#34;https://user-images.githubusercontent.com/3881276/292949063-1e627051-2949-4056-8edf-838afe102fb0.png&#34; /&gt;&#xA;        &lt;/a&gt;&#xA;    &lt;/p&gt;</description>
    </item>
    <item>
      <title>新加入 Azure AI Model Catalog 的模型</title>
      <link>https://beanhsiang.github.io/post/2023-11-16-more-to-azure-ai-model-catalog/</link>
      <pubDate>Thu, 16 Nov 2023 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2023-11-16-more-to-azure-ai-model-catalog/</guid>
      <description>&lt;p&gt;我们很高兴地看到，在 Azure AI Model Catalog 中增加了几个新的基础和生成AI模型。从 Hugging Face 我们已经推出了一系列稳定的 stable diffusion 模型，falcon 模型，CLIP, Whisper V3, BLIP 和 SAM 模型。除了 Hugging Face 模型，我们还分别添加了 Meta 和 NVIDIA 的 Code Llama 和 Nemotron 模型。我们还将介绍微软研究中心的尖端 Phi 模型。Model Catalog 产生了40个新模型和4个新模式，包括文本到图像和图像嵌入。专业开发人员很快就可以轻松地将最新的 AI 模型(如Meta 的 Llama 2、Cohere 的 Command、G42 的 Jais 以及 Mistral 的高级模型)作为 API 集成到他们的应用程序中。他们还可以使用自己的数据对这些模型进行微调，而无需担心设置和管理GPU基础设施，从而帮助消除配置资源和管理主机的复杂性。&lt;/p&gt;</description>
    </item>
    <item>
      <title>使用 GitHub Copilot 的自动错误检测和纠正增强调试</title>
      <link>https://beanhsiang.github.io/post/2023-06-07-enhancing-debugging-with-github-copilot/</link>
      <pubDate>Wed, 07 Jun 2023 12:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2023-06-07-enhancing-debugging-with-github-copilot/</guid>
      <description>&lt;h2 id=&#34;github-copilot-的自动错误检测和纠正如何帮助简化调试&#34;&gt;GitHub Copilot 的自动错误检测和纠正如何帮助简化调试&lt;/h2&gt;&#xA;&lt;p&gt;对于软件开发人员来说，调试可能是一个繁琐且耗时的过程。它需要仔细分析代码以识别和修复错误，并且通常需要数小时或数天才能完成。幸运的是，GitHub Copilot 开发了一个自动错误检测和纠正系统，可以帮助简化调试过程。&lt;/p&gt;&#xA;&lt;p&gt;GitHub Copilot 的自动错误检测和纠正系统使用机器学习算法来识别和修复代码中的错误。它可以检测语法和逻辑中的错误，还可以检测和纠正编码风格问题。该系统可用于检测任何编程语言（包括 JavaScript、Python 和 Java）中的错误。&lt;/p&gt;</description>
    </item>
    <item>
      <title>ML.NET Model Builder 中的对象检测</title>
      <link>https://beanhsiang.github.io/post/2023-05-20-object-detection-ml-dotnet-model-builder/</link>
      <pubDate>Sat, 20 May 2023 12:20:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2023-05-20-object-detection-ml-dotnet-model-builder/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://dot.net/ml?WT.mc_id=AI-MVP-5003172&#34;&gt;ML.NET&lt;/a&gt; 是一个面向 .NET 开发人员的开源跨平台机器学习框架，可将自定义机器学习模型集成到 .NET 应用中。&lt;/p&gt;&#xA;&lt;p&gt;新版本的 &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=MLNET.ModelBuilder2022&#34;&gt;Model Builder&lt;/a&gt; 现已推出，其中包括对使用本地 CPU 或 GPU 进行对象检测的支持。&lt;/p&gt;&#xA;&lt;h2 id=&#34;什么是对象检测&#34;&gt;什么是对象检测？&lt;/h2&gt;&#xA;&lt;p&gt;物体检测是一个计算机视觉问题。虽然与图像分类密切相关，但对象检测以更精细的规模执行图像分类。对象检测可对图像中的实体进行定位和分类。当图像包含多个不同类型的对象时，使用对象检测。&lt;/p&gt;</description>
    </item>
    <item>
      <title>基于 LangChain 实现问答</title>
      <link>https://beanhsiang.github.io/post/2023-04-11-question-answering-in-langchain/</link>
      <pubDate>Tue, 11 Apr 2023 14:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2023-04-11-question-answering-in-langchain/</guid>
      <description>&lt;p&gt;您是否有兴趣与自己的文档聊天，无论是文本文件、PDF 还是网站？LangChain 使您可以轻松地使用文档进行问答。在这篇博文中，我们将探讨四种不同的问答方法，以及您可以为案例考虑的各种选项。&lt;/p&gt;&#xA;&lt;p&gt;在我们实现问答之前，您可能想知道：什么是 &lt;a href=&#34;https://python.langchain.com/en/latest/&#34;&gt;LangChain&lt;/a&gt;？简单来说，LangChain 是一种快速与语言模型交互和构建应用程序的开源框架，方便集成许多主流的 LLM 和组件扩展。&lt;/p&gt;</description>
    </item>
    <item>
      <title>ML.NET Model Builder 中的句子相似性</title>
      <link>https://beanhsiang.github.io/post/2023-03-02-sentence-similarity-mlnet-model-builder/</link>
      <pubDate>Thu, 02 Mar 2023 13:23:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2023-03-02-sentence-similarity-mlnet-model-builder/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://dot.net/ml?WT.mc_id=AI-MVP-5003172&#34;&gt;ML.NET&lt;/a&gt; 是一个面向 .NET 开发人员的开源跨平台机器学习框架，可将自定义机器学习模型集成到 .NET 应用中。&lt;/p&gt;&#xA;&lt;p&gt;新版本的 &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=MLNET.ModelBuilder2022&#34;&gt;Model Builder&lt;/a&gt; 现已发布！&lt;/p&gt;&#xA;&lt;h2 id=&#34;有什么新变化&#34;&gt;有什么新变化？&lt;/h2&gt;&#xA;&lt;p&gt;以下是此版本的亮点。您可以在 &lt;a href=&#34;https://github.com/dotnet/machinelearning-modelbuilder/blob/main/docs/release-notes/16.14.4.md&#34;&gt;Model Builder 发行说明&lt;/a&gt;中找到所有更改的列表。&lt;/p&gt;&#xA;&lt;p&gt;要开始使用这些新功能，请&lt;a href=&#34;https://learn.microsoft.com/dotnet/machine-learning/how-to-guides/install-model-builder?tabs=visual-studio-2022&amp;amp;WT.mc_id=AI-MVP-5003172&#34;&gt;安装或升级到&lt;/a&gt;最新版本的 Model Builder 16.14.4 或更高版本。&lt;/p&gt;&#xA;&lt;h3 id=&#34;model-builder中的句子相似性&#34;&gt;Model Builder中的句子相似性&lt;/h3&gt;&#xA;&lt;p&gt;句子相似性是一项比较两个文本彼此相似程度的任务。&lt;/p&gt;&#xA;&lt;p&gt;句子相似性的常见用例是信息检索。例如，给出一个搜索查询，返回最相似（相关）的文档。&lt;/p&gt;</description>
    </item>
    <item>
      <title>在 Azure ML 中引入可缩放的企业级基因学工作流</title>
      <link>https://beanhsiang.github.io/post/2023-02-25-introducing-scalable-and-enterprise-grade-genomics-workflows-in/</link>
      <pubDate>Sun, 26 Feb 2023 11:15:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2023-02-25-introducing-scalable-and-enterprise-grade-genomics-workflows-in/</guid>
      <description>&lt;p&gt;基因学工作流程在生物信息学中至关重要，因为它们可以帮助研究人员分析和解释大量的基因组数据。但是，使用专用软件和复杂的依赖项创建一致且可重复的环境可能具有挑战性，因此与 CI/CD 工具的集成也很困难。&lt;/p&gt;&#xA;&lt;p&gt;Azure 机器学习 （Azure ML） 是一个基于云的平台，提供一组全面的工具和服务，用于开发、部署和管理机器学习模型。Azure ML 原生提供出色的可重复性和可审核性功能，而没有多少工作流解决方案提供这些功能。它为运行工作流提供了一个高度集成和标准化的环境，确保每个步骤都以一致且可重复的方式执行。此功能对于需要使用具有特定依赖项的某些版本的多个工具和软件包的基因学工作流特别有用。&lt;/p&gt;</description>
    </item>
    <item>
      <title>3个加速语言学习的微软 Azure AI 产品功能</title>
      <link>https://beanhsiang.github.io/post/2023-02-25-3-microsoft-azure-ai-product-features-that-accelerate-language-learning/</link>
      <pubDate>Sat, 25 Feb 2023 09:12:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2023-02-25-3-microsoft-azure-ai-product-features-that-accelerate-language-learning/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://azure.microsoft.com/products/cognitive-services/speech-services&#34;&gt;Microsoft Azure 认知语音服务&lt;/a&gt;平台是技术和服务的综合集合，旨在加速将语音整合到应用程序中，从而扩大市场差异化。可用的服务包括语音转文本、文本转语音、自定义神经语音 （CNV） 对话听录服务、说话人识别、语音翻译、语音 SDK 和语音设备开发工具包 （DDK）。&lt;/p&gt;&#xA;&lt;p&gt;人工智能教育是一项新兴技术，有可能彻底改变我们教授和学习语言的方式。语言学习最重要的方面之一是准确发音的能力，这就是 Azure 认知语音服务的新&lt;a href=&#34;https://learn.microsoft.com/azure/cognitive-services/speech-service/how-to-pronunciation-assessment?pivots=programming-language-python&#34;&gt;发音评估&lt;/a&gt;功能的用武之地。另一个关键机会是开发合成双语语音，用于使用自定义神经语音进行语言学习体验，以及我们的语音转文本功能。&lt;/p&gt;</description>
    </item>
    <item>
      <title>训练自定义表单识别器并对其进行基准测试</title>
      <link>https://beanhsiang.github.io/post/2023-02-21-train-and-benchmark-a-custom-forms-recognizer-using-forms/</link>
      <pubDate>Tue, 21 Feb 2023 09:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2023-02-21-train-and-benchmark-a-custom-forms-recognizer-using-forms/</guid>
      <description>&lt;p&gt;&#xA;        &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://user-images.githubusercontent.com/3881276/224236800-0136cc55-510b-497f-a5e0-1c1d5c0eae74.png&#34;&gt;&#xA;            &lt;img class=&#34;mx-auto&#34; alt=&#34;image&#34; src=&#34;https://user-images.githubusercontent.com/3881276/224236800-0136cc55-510b-497f-a5e0-1c1d5c0eae74.png&#34; /&gt;&#xA;        &lt;/a&gt;&#xA;    &lt;/p&gt;&#xA;&lt;h2 id=&#34;步骤概述&#34;&gt;步骤概述&lt;/h2&gt;&#xA;&lt;p&gt;定制的 OCR 解决方案提供了在文档或图像中定义唯一类别的功能。通过与各种客户合作开发定制 OCR 解决方案，我们经常听到这样的问题：“此解决方案在我的数据上表现如何？我们开发了一种方法，允许使用&lt;a href=&#34;https://formrecognizer.appliedai.azure.com/studio&#34;&gt;表单识别器工作室&lt;/a&gt;根据自定义数据对 &lt;a href=&#34;https://azure.microsoft.com/en-us/products/form-recognizer/&#34;&gt;Microsoft 的表单识别器&lt;/a&gt;进行基准测试，并在一个过程中使用基本事实批注训练自定义模型。&lt;/p&gt;&#xA;&lt;h2 id=&#34;使用表单识别器工作室批注基本事实&#34;&gt;使用表单识别器工作室批注基本事实&lt;/h2&gt;&#xA;&lt;p&gt;在训练自定义表单识别器模型之前，必须具有标记或批注的数据集，也称为基本事实。为了提供注释过程的示例，我们创建了一个扫描的手写邮政地址的示例图像。真实名称为“John Doe”，地址为“000 Fifth Ave， NY 10065， USA”，如下图所示：&lt;/p&gt;</description>
    </item>
    <item>
      <title>使用 Intel oneDAL 加速 ML.NET 训练</title>
      <link>https://beanhsiang.github.io/post/2022-12-23-accelerate-ml-net-training-with-intel-onedal/</link>
      <pubDate>Fri, 23 Dec 2022 12:24:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2022-12-23-accelerate-ml-net-training-with-intel-onedal/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://dot.net/ml?WT.mc_id=AI-MVP-5003172&#34;&gt;ML.NET&lt;/a&gt; 是一个面向 .NET 开发人员的开源跨平台机器学习框架，可将自定义机器学习模型集成到 .NET 应用中。&lt;/p&gt;&#xA;&lt;p&gt;ML.NET 3.0 的第一个预览版带来了多项硬件加速改进，使你可以在训练期间充分利用计算资源。安装最新的 &lt;a href=&#34;https://aka.ms/mlnet-3-preview1&#34;&gt;ML.NET 3.0&lt;/a&gt; 和&lt;a href=&#34;https://aka.ms/mlnet-onedal-nuget&#34;&gt;Intel oneDAL&lt;/a&gt; 预览包，试用由 Intel oneDAL 提供支持的最新改进。&lt;/p&gt;&#xA;&lt;h2 id=&#34;什么是-oneapi-data-analytics-libraryonedal&#34;&gt;什么是 oneAPI Data Analytics Library（oneDAL）&lt;/h2&gt;&#xA;&lt;p&gt;Intel oneAPI Data Analytics Library 是一个通过为数据分析和机器学习过程的所有阶段提供高度优化的算法构建模块，帮助加快数据分析速度的库。oneDAL 利用 64 位架构中的 SIMD 扩展，这些扩展在 Intel 和 AMD CPU 中都有。&lt;/p&gt;</description>
    </item>
    <item>
      <title>AI 预测你在视频中可能购买哪些服装</title>
      <link>https://beanhsiang.github.io/post/2022-12-10-the-ai-technology-that-predicts-which-clothing-items-appearing/</link>
      <pubDate>Sat, 10 Dec 2022 09:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2022-12-10-the-ai-technology-that-predicts-which-clothing-items-appearing/</guid>
      <description>&lt;p&gt;Azure 视频索引器很高兴地宣布推出一项名为“特色服装”的新功能，该功能现已推出公共预览版。&lt;/p&gt;&#xA;&lt;p&gt;借助此功能，发布商和广告主可以利用深入的上下文理解，在观看者最容易接受与广告互动的最相关时间投放最相关的广告。&lt;/p&gt;&#xA;&lt;p&gt;这种洞察力使用先进的人工智能来深入了解关键时刻、主要角色、重要场景和所显示的情感，所有这些都是为了识别视频中出现的关键服装项目。对于这些服装商品，我们会提供它们出现的确切帧以及更多信息（例如边界框和时间码），这些信息可以帮助广告主和发布商匹配类似服装商品的相关广告，并将其放置在该商品在视频中展示的确切时刻。&lt;/p&gt;</description>
    </item>
    <item>
      <title>ML.NET 2.0 更新内容</title>
      <link>https://beanhsiang.github.io/post/2022-11-20-mlnet-2_0-content/</link>
      <pubDate>Sun, 20 Nov 2022 18:12:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2022-11-20-mlnet-2_0-content/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://dot.net/ml?WT.mc_id=AI-MVP-5003172&#34;&gt;ML.NET&lt;/a&gt; v2.0 比较低调地在11月发布了，所有 &lt;a href=&#34;https://github.com/dotnet/machinelearning/blob/main/docs/release-notes/2.0/release-2.0.0.md&#34;&gt;ML.NET 2.0&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/dotnet/machinelearning-modelbuilder/blob/main/docs/release-notes/16.14.0.md&#34;&gt;Model Builder&lt;/a&gt; 的更新信息在此，可以从中了解到更多细节或实现，本文把值得关注的内容摘要出来。&lt;/p&gt;&#xA;&lt;p&gt;新版本的 &lt;a href=&#34;https://www.nuget.org/packages/Microsoft.ML/2.0.0&#34;&gt;ML.NET 2.0&lt;/a&gt; 和 &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=MLNET.ModelBuilder2022&#34;&gt;Model Builder 2022&lt;/a&gt; 目前都已发布。&lt;/p&gt;&#xA;&lt;h2 id=&#34;model-builder-中的文本分类场景&#34;&gt;Model Builder 中的文本分类场景&lt;/h2&gt;&#xA;&lt;p&gt;几个月前发布的文本分类 API 的预览版，使开发者能够训练对原始文本数据进行分类的自定义模型。它通过将 &lt;a href=&#34;https://arxiv.org/abs/2105.14444&#34;&gt;NAS-BERT&lt;/a&gt; 的 &lt;a href=&#34;https://github.com/dotnet/TorchSharp&#34;&gt;TorchSharp&lt;/a&gt; 实现集成到 ML.NET 中来实现。文本分类 API 基于此模型的预训练版本，使用数据来微调模型。Model Builder 中的文本分类场景正式由 ML.NET 文本分类 API 提供支持。&lt;/p&gt;</description>
    </item>
    <item>
      <title>使用 Azure 表单识别器生成可搜索的 PDF</title>
      <link>https://beanhsiang.github.io/post/2022-10-18-generate-searchable-pdfs-with-azure-form-recognizer/</link>
      <pubDate>Tue, 18 Oct 2022 11:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2022-10-18-generate-searchable-pdfs-with-azure-form-recognizer/</guid>
      <description>&lt;p&gt;PDF文档广泛用于业务流程。数字创建的 PDF 使用起来非常方便。可以搜索、突出显示和批注文本。不幸的是，许多PDF是通过扫描图像或将图像转换为PDF来创建的。这些 PDF 中没有数字文本，因此无法搜索它们。在这篇博文中，我们演示了如何使用简单易用的代码和 Azure 表单识别器将此类 PDF 转换为可搜索的 PDF。&lt;/p&gt;&#xA;&lt;h2 id=&#34;azure-表单识别器概述&#34;&gt;Azure 表单识别器概述&lt;/h2&gt;&#xA;&lt;p&gt;Azure 表单识别器是一种基于云的 Azure 应用 AI 服务，它使用深度机器学习模型从文档中提取文本、键值对、表和表单字段。在这篇博文中，我们将使用表单识别器提取的文本将其添加到 PDF 中，使其可搜索。&lt;/p&gt;</description>
    </item>
    <item>
      <title>利用 AutomatedML 模型背后的训练代码</title>
      <link>https://beanhsiang.github.io/post/2022-10-14-make-use-of-the-training-code-behind-your-automatedml-model/</link>
      <pubDate>Fri, 14 Oct 2022 11:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2022-10-14-make-use-of-the-training-code-behind-your-automatedml-model/</guid>
      <description>&lt;h2 id=&#34;概述&#34;&gt;概述&lt;/h2&gt;&#xA;&lt;p&gt;我们很高兴地宣布正式发布自动化机器学习 （AutoML） 训练代码生成。借助此功能，用户可以查看其 AutoML 模型背后的训练脚本，以确保他们对模型的训练方式完全透明。用户还可以使用该脚本根据其用例的需要自定义/调整训练，从而使他们能够快速将 AutoML 模型转移到生产环境中。&lt;/p&gt;&#xA;&lt;h2 id=&#34;为什么这很重要&#34;&gt;为什么这很重要？&lt;/h2&gt;&#xA;&lt;p&gt;AutoML对于数据科学家来说是一个非常强大的工具。他们需要做的就是提供数据并配置基本作业参数，AutoML 会迭代适用的 ML 算法，以根据提供的数据和所选的准确性指标训练理想的模型。然而，我们经常听到 AutoML 是一个黑匣子——禁止数据科学家生产 AutoML 模型。&lt;/p&gt;</description>
    </item>
    <item>
      <title>在 Azure Machine Learning 中使用 Azure Container for PyTorch 启用深度学习</title>
      <link>https://beanhsiang.github.io/post/2022-10-13-enabling-deep-learning-with-azure-container-for-pytorch-in-azure/</link>
      <pubDate>Thu, 13 Oct 2022 11:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2022-10-13-enabling-deep-learning-with-azure-container-for-pytorch-in-azure/</guid>
      <description>&lt;h2 id=&#34;概述&#34;&gt;概述&lt;/h2&gt;&#xA;&lt;p&gt;由于AzureML是许多PyTorch开发人员的首选平台，我们开发了新的Azure Container for PyTorch（ACPT），这是一个精心策划的环境，包括最好的Microsoft技术，用于在Azure上使用PyTorch进行训练。我们很高兴地宣布 Azure 机器学习 （AzureML） 中的 ACPT 公共预览版。这个新的特选环境是一个轻量级的独立环境，其中包含在 AzureML 上有效运行大型模型优化训练所需的组件。默认情况下，AzureML 特选环境在用户的工作区中可用，并由使用最新版本的 AzureML SDK 的缓存 Docker 映像提供支持。它有助于降低准备成本并缩短部署时间。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Azure Cognitive Search 支持交换索引</title>
      <link>https://beanhsiang.github.io/post/2022-07-02-azure-cognitive-search-now-supports-swapping-indexes/</link>
      <pubDate>Sat, 02 Jul 2022 10:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2022-07-02-azure-cognitive-search-now-supports-swapping-indexes/</guid>
      <description>&lt;p&gt;搜索索引是不可变的，这意味着如果需要更新索引，通常需要&lt;a href=&#34;https://docs.microsoft.com/azure/search/search-howto-reindex?WT.mc_id=AI-MVP-5003172&#34;&gt;删除并重新生成&lt;/a&gt;索引或创建新索引，然后迁移应用程序。像这样交换搜索索引会增加管理搜索索引的复杂性，如果最终需要在多个位置更新应用程序设置或需要重新部署代码，则可能会很困难。&lt;/p&gt;&#xA;&lt;p&gt;为了帮助更轻松地交换索引，Azure 认知搜索现在支持预览版提供的&lt;a href=&#34;https://docs.microsoft.com/azure/search/search-how-to-alias?tabs=rest&amp;amp;WT.mc_id=AI-MVP-5003172&#34;&gt;索引别名&lt;/a&gt;。别名是辅助名称，可用于引用索引以进行查询、索引和其他文档操作。您可以通过 &lt;a href=&#34;https://docs.microsoft.com/rest/api/searchservice/preview-api/create-or-update-alias?WT.mc_id=AI-MVP-5003172&#34;&gt;REST API&lt;/a&gt;、&lt;a href=&#34;https://docs.microsoft.com/azure/search/search-how-to-alias?tabs=vscode&amp;amp;WT.mc_id=AI-MVP-5003172#create-an-index-alias&#34;&gt;Visual Studio Code 扩展&lt;/a&gt;或 &lt;a href=&#34;https://docs.microsoft.com/dotnet/api/azure.search.documents.indexes.searchindexclient.createorupdatealias?WT.mc_id=AI-MVP-5003172&amp;amp;view=azure-dotnet-preview#azure-search-documents-indexes-searchindexclient-createorupdatealias(system-string-azure-search-documents-indexes-models-searchalias-system-boolean-system-threading-cancellationtoken)&#34;&gt;SDK&lt;/a&gt; 创建别名。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;POST https://[service name].search.windows.net/aliases?api-version=2021-04-30-Preview&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;div style=&#34;background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&#xA;&lt;table style=&#34;border-spacing:0;padding:0;margin:0;border:0;&#34;&gt;&lt;tr&gt;&lt;td style=&#34;vertical-align:top;padding:0;margin:0;border:0;&#34;&gt;&#xA;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;1&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;2&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;3&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;4&#xA;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&#xA;&lt;td style=&#34;vertical-align:top;padding:0;margin:0;border:0;;width:100%&#34;&gt;&#xA;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;{&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   &lt;span style=&#34;color:#0550ae&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;my-alias&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;,&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   &lt;span style=&#34;color:#0550ae&#34;&gt;&amp;#34;indexes&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#1f2328&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#0a3069&#34;&gt;&amp;#34;hotel-samples-index&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;]&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#1f2328&#34;&gt;}&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&#xA;&lt;/div&gt;&#xA;&lt;/div&gt;&lt;p&gt;如果您发现自己相当频繁地更新搜索索引，则可能会从使用别名中受益。下图显示了在应用程序中使用索引别名的典型工作流。&#xA;&#xA;        &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://user-images.githubusercontent.com/3881276/224216713-4a1e6d70-0277-46cd-832a-1194d5decd2d.png&#34;&gt;&#xA;            &lt;img class=&#34;mx-auto&#34; alt=&#34;image&#34; src=&#34;https://user-images.githubusercontent.com/3881276/224216713-4a1e6d70-0277-46cd-832a-1194d5decd2d.png&#34; /&gt;&#xA;        &lt;/a&gt;&#xA;    &lt;/p&gt;</description>
    </item>
    <item>
      <title>Azure 机器学习与 H2O.ai 的新集成</title>
      <link>https://beanhsiang.github.io/post/2022-06-22-azure-machine-learning-s-new-integration-with-h2o-ai/</link>
      <pubDate>Wed, 22 Jun 2022 10:00:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2022-06-22-azure-machine-learning-s-new-integration-with-h2o-ai/</guid>
      <description>&lt;p&gt;在当今复杂的云环境中，公司使用来自多个来源的技术是很常见的。云计算平台通常是企业IT环境的基础，由独立软件供应商和满足业务和工业垂直需求的产品补充。通过 H2O.ai 和 Azure 机器学习之间的集成，我们使客户能够使用类似 SaaS 的工具实现模型创建民主化，并选择符合其企业要求的部署技术。&lt;/p&gt;&#xA;&lt;p&gt;使用 H2O.ai 的 AzureML 集成，H2O.ai 中构建的模型现在显示为 AzureML 工作区中的已部署模型。这意味着任何用户或产品现在都可以通过简单的 API 调用利用 AzureML 中托管的 H2O.ai 模型的推理终结点。这使得组织模型的采用更加容易，允许用户访问整个企业 IT 环境中的模型，而无需更改现有的部署策略。&lt;/p&gt;</description>
    </item>
    <item>
      <title>有关 ML.NET 深度学习的计划</title>
      <link>https://beanhsiang.github.io/post/2021-09-21-mlnet-deep-learning-plans/</link>
      <pubDate>Tue, 21 Sep 2021 16:43:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2021-09-21-mlnet-deep-learning-plans/</guid>
      <description>&lt;p&gt;ML.NET 最需要的功能之一是能够从零开始创建神经网络模型，以便在 ML.NET 中进行深度学习。ML.NET 团队已经采纳了用户调研的反馈，并制定了开始实施此功能的计划。&lt;/p&gt;&#xA;&lt;h2 id=&#34;mlnet-深度学习的现状&#34;&gt;ML.NET 深度学习的现状&lt;/h2&gt;&#xA;&lt;p&gt;目前，ML.NET 没有一种方法可以创建神经网络，从零开始建立深度学习模型。然而，对于采用现有的深度学习模型并将其用于预测，框架给予极大的支持。如果您有 TensorFlow 或 ONNX 模型，则这些模型可用于 ML.NET 进行预测。&lt;/p&gt;</description>
    </item>
    <item>
      <title>ML.NET 代码速查手册，高清图手慢无！</title>
      <link>https://beanhsiang.github.io/post/2021-09-02-mlnet-cheatsheet/</link>
      <pubDate>Wed, 01 Sep 2021 22:15:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2021-09-02-mlnet-cheatsheet/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://dot.net/ml?WT.mc_id=AI-MVP-5003172&#34;&gt;ML.NET&lt;/a&gt; 是面向.NET开发人员的跨平台机器学习框架，它具备在联机或本地环境中将机器学习模型集成到 .NET 应用程序中的能力。&lt;/p&gt;&#xA;&lt;p&gt;&#xA;        &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://user-images.githubusercontent.com/3881276/131783402-fb40f512-19a4-46a7-b837-78ac13aaba74.jpeg&#34;&gt;&#xA;            &lt;img class=&#34;mx-auto&#34; alt=&#34;ML.NET&#34; src=&#34;https://user-images.githubusercontent.com/3881276/131783402-fb40f512-19a4-46a7-b837-78ac13aaba74.jpeg&#34; /&gt;&#xA;        &lt;/a&gt;&#xA;    &lt;/p&gt;&#xA;&lt;p&gt;2002 年微软启动有一个研究项目命名为 TMSN，其意在“Test mining search and navigation”，后来它被改名为 TLC（The learning code）。ML.NET 正是派生自 TLC 库，最初被用于微软的内部产品。&lt;/p&gt;</description>
    </item>
    <item>
      <title>ML.NET v1.6 中有趣的内容</title>
      <link>https://beanhsiang.github.io/post/2021-08-07-mlnet-1_6-interesting-content/</link>
      <pubDate>Sat, 07 Aug 2021 19:53:00 +0800</pubDate>
      <guid>https://beanhsiang.github.io/post/2021-08-07-mlnet-1_6-interesting-content/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://dot.net/ml?WT.mc_id=AI-MVP-5003172&#34;&gt;ML.NET&lt;/a&gt; v1.6 发布有一段时间了，所有&lt;a href=&#34;https://github.com/dotnet/machinelearning/blob/main/docs/release-notes/1.6.0/release-1.6.0.md&#34;&gt;详细信息&lt;/a&gt;在此，可以了解到更多细节或实现，本文把比较新鲜有趣的内容摘要出来。&lt;/p&gt;&#xA;&lt;p&gt;此版本中增加了很多东西，但它们确实注意到，添加的所有内容都没有中断更改。&lt;/p&gt;&#xA;&lt;h3 id=&#34;支持-arm&#34;&gt;支持 ARM&lt;/h3&gt;&#xA;&lt;p&gt;这次更新最令人兴奋的部分是&lt;a href=&#34;https://github.com/dotnet/machinelearning/pull/5789&#34;&gt;对ARM架构的新支持&lt;/a&gt;。这将允许在 &lt;a href=&#34;https://dot.net/ml?WT.mc_id=AI-MVP-5003172&#34;&gt;ML.NET&lt;/a&gt; 进行大多数场景的训练和推论项目。ARM架构几乎无处不在。如&lt;a href=&#34;https://devblogs.microsoft.com/dotnet/ml-net-june-updates-model-builder/?WT.mc_id=AI-MVP-5003172#ml-net-on-arm&#34;&gt;六月更新博客&lt;/a&gt;文章中所述，此 ARM 架构包含在移动和嵌入式设备上。这可以为手机和 IoT 设备的 &lt;a href=&#34;https://dot.net/ml?WT.mc_id=AI-MVP-5003172&#34;&gt;ML.NET&lt;/a&gt; 开辟全新的世界。&lt;/p&gt;&#xA;&lt;h3 id=&#34;dataframe-更新&#34;&gt;DataFrame 更新&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.nuget.org/packages/Microsoft.Data.Analysis/&#34;&gt;DataFrame API&lt;/a&gt; 可能是目前处于早期阶段的特性之一。主要还是由于 .NET 与 Python 中常见库 pandas 在数据分析上面不存在竞争，在将数据发送到 &lt;a href=&#34;https://dot.net/ml?WT.mc_id=AI-MVP-5003172&#34;&gt;ML.NET&lt;/a&gt; 进行建模之前，你可能需要处理一些预处理。现在 DataFrame API 已经进入 &lt;a href=&#34;https://github.com/dotnet/machinelearning/pull/5641&#34;&gt;ML.NET 主库&lt;/a&gt; ，该代码以前作为实验包在 &lt;a href=&#34;https://github.com/dotnet/corefxlab&#34;&gt;CoreFx Lab&lt;/a&gt;存储库中，但现在它不再是实验性的，现在已是 &lt;a href=&#34;https://dot.net/ml?WT.mc_id=AI-MVP-5003172&#34;&gt;ML.NET&lt;/a&gt; 的一部分意味着项目计划持续更新。&lt;/p&gt;</description>
    </item>
    <item>
      <title>关于我</title>
      <link>https://beanhsiang.github.io/about/</link>
      <pubDate>Tue, 15 Jun 2021 10:08:43 +0800</pubDate>
      <guid>https://beanhsiang.github.io/about/</guid>
      <description>&lt;h2 id=&#34;pytorch机器学习从入门到实战联名作者&#34;&gt;《PyTorch机器学习从入门到实战》联名作者&lt;/h2&gt;&#xA;&lt;p&gt;&#xA;        &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://user-images.githubusercontent.com/3881276/293570908-3b161277-b1ef-4d87-9818-274228e9c634.png&#34;&gt;&#xA;            &lt;img class=&#34;mx-auto&#34; alt=&#34;PyTorch&#34; src=&#34;https://user-images.githubusercontent.com/3881276/293570908-3b161277-b1ef-4d87-9818-274228e9c634.png&#34; /&gt;&#xA;        &lt;/a&gt;&#xA;    &lt;/p&gt;&#xA;&lt;h2 id=&#34;微软-mvpai&#34;&gt;&lt;a href=&#34;https://mvp.microsoft.com/zh-CN/search?target=Profile&amp;amp;program=MVP&#34;&gt;微软 MVP（AI）&lt;/a&gt;&lt;/h2&gt;</description>
    </item>
    <item>
      <title>归档</title>
      <link>https://beanhsiang.github.io/archives/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://beanhsiang.github.io/archives/</guid>
      <description></description>
    </item>
    <item>
      <title>搜索</title>
      <link>https://beanhsiang.github.io/search/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://beanhsiang.github.io/search/</guid>
      <description></description>
    </item>
  </channel>
</rss>
